{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581c6cab",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189bd59a",
   "metadata": {},
   "source": [
    "After evaluating the performance of different modelling frameworks with baseline hyperparameters, the performance of the Gradiendt Boosting Classifier showed the highest accuracy score. Hence, we are proceeding with the Gradient Boosting Classifier (GBC), or the `sklearn` class `GradientBoostingClassifier()`.\n",
    "\n",
    "### Gradient Boosting Classifier\n",
    "#### What is Gradient Boosting?\n",
    "Before we start examining the hyperparameters of the Gradient Boosting Classifier (`GradientBoostingClassifier()`), it would be a good idea to recap on what this classifier is. So what is GBC? According to [this source](https://medium.com/analytics-vidhya/introduction-to-gradient-boosting-classification-da4e81f54d3):\n",
    "\n",
    ">Gradient Boosting is the grouping of gradient descent and boosting. In gradient boosting, each new model minimizes the loss function from its predecessor using the Gradient Descent Method. This procedure continues until a more optimal estimate of the target variable has been achieved.\n",
    "\n",
    "GBC, therefore, is a sequential ensemble model - meaning that in order to achieve great accuracy, a number of models need to run. Importantly, this cannot be done in parallel (hence, 'sequential'). This is important to note, as GBCs can get computationally costly, especially in a large hyperparameter space.\n",
    "\n",
    "#### Hyperparameters of GBC (and `GradientBoostingClassifier()`)\n",
    "The Hyperparameters of GBC are quite neatly covered in the [previously-used source](https://medium.com/analytics-vidhya/introduction-to-gradient-boosting-classification-da4e81f54d3). Here is a brief overview of most important hyperparameters:\n",
    "\n",
    "- **Learning rate**: by using gradient descent, this hyperparameter affects the speed of algorithm convergence. There is a tradeoff between the value size with smaller values minimizing overfitting (but taking more time until convergence) and larger values, while potentially not reaching an optimal fit, lead to a faster convergence.\n",
    "\n",
    "- **Number of trees**: how many trees are optimal to minimize the loss function? \n",
    "\n",
    "- **Depth of trees**: the number of splitsin a tree.\n",
    "\n",
    "- **Subsampling**: what is the proportion of trainig data used for training a tree? \n",
    "\n",
    "All this briefly summarized, let's start the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518c6e7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065c3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "#!pip install hyperopt\n",
    "#!pip install git+https://github.com/hyperopt/hyperopt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69141ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "## General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "## Model training and evaluation\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Classifiers\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "## Hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import hyperopt.pyll.stochastic\n",
    "\n",
    "## Misc\n",
    "from statsmodels.stats.contingency_tables import mcnemar # McNemar test\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# About FMIN: https://github.com/hyperopt/hyperopt/wiki/FMin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29786a20",
   "metadata": {},
   "source": [
    "## Import the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "989bbdb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aluminium;ammonium;boron;chloride;coli-like-bacteria-colilert;coli-like-bacteria;colony-count-at-22-c;color-pt-co-unit;color-pt/co-scale;electrical-conductivity;enterococci;escherichia-coli-colilert;escherichia-coli;fluoride;iron;manganese;nitrate;nitrite;odour-dilution-level;oxidability;smell-ball-units;sodium;sulphate;taste-ball-units;taste-dilution-degree;turbidity-ntu;ph ;compliance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0;0.05;0.2;23.0;0.0;0.0;5.0;1.8;2.1;474.0;0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0;0.05;0.2;23.0;0.0;0.0;0.0;5.0;0.0;465.0;0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0;0.05;0.2;23.0;0.0;0.0;0.0;5.0;0.0;448.0;0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0;0.09;0.641;23.0;0.0;0.0;15.0;5.0;4.0;978.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0;0.06;0.2;23.0;0.0;0.0;1.0;6.6;2.1;446.0;0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>5.0;0.05;0.2;23.0;0.0;0.0;3.0;5.0;3.0;622.0;0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>5.0;0.05;0.2;23.0;0.0;0.0;10.0;5.0;5.0;500.0;0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>5.0;0.27;0.274;14.0;0.0;0.0;66.0;5.0;6.0;596.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>5.0;0.05;0.2;23.0;0.0;0.0;1.0;2.7;2.1;425.0;0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>5.0;0.05;0.2;23.0;0.0;0.0;18.0;5.0;3.0;603.0;0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>880 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aluminium;ammonium;boron;chloride;coli-like-bacteria-colilert;coli-like-bacteria;colony-count-at-22-c;color-pt-co-unit;color-pt/co-scale;electrical-conductivity;enterococci;escherichia-coli-colilert;escherichia-coli;fluoride;iron;manganese;nitrate;nitrite;odour-dilution-level;oxidability;smell-ball-units;sodium;sulphate;taste-ball-units;taste-dilution-degree;turbidity-ntu;ph ;compliance\n",
       "0    5.0;0.05;0.2;23.0;0.0;0.0;5.0;1.8;2.1;474.0;0....                                                                                                                                                                                                                                                                                                                                                   \n",
       "1    5.0;0.05;0.2;23.0;0.0;0.0;0.0;5.0;0.0;465.0;0....                                                                                                                                                                                                                                                                                                                                                   \n",
       "2    5.0;0.05;0.2;23.0;0.0;0.0;0.0;5.0;0.0;448.0;0....                                                                                                                                                                                                                                                                                                                                                   \n",
       "3    5.0;0.09;0.641;23.0;0.0;0.0;15.0;5.0;4.0;978.0...                                                                                                                                                                                                                                                                                                                                                   \n",
       "4    5.0;0.06;0.2;23.0;0.0;0.0;1.0;6.6;2.1;446.0;0....                                                                                                                                                                                                                                                                                                                                                   \n",
       "..                                                 ...                                                                                                                                                                                                                                                                                                                                                   \n",
       "875  5.0;0.05;0.2;23.0;0.0;0.0;3.0;5.0;3.0;622.0;0....                                                                                                                                                                                                                                                                                                                                                   \n",
       "876  5.0;0.05;0.2;23.0;0.0;0.0;10.0;5.0;5.0;500.0;0...                                                                                                                                                                                                                                                                                                                                                   \n",
       "877  5.0;0.27;0.274;14.0;0.0;0.0;66.0;5.0;6.0;596.0...                                                                                                                                                                                                                                                                                                                                                   \n",
       "878  5.0;0.05;0.2;23.0;0.0;0.0;1.0;2.7;2.1;425.0;0....                                                                                                                                                                                                                                                                                                                                                   \n",
       "879  5.0;0.05;0.2;23.0;0.0;0.0;18.0;5.0;3.0;603.0;0...                                                                                                                                                                                                                                                                                                                                                   \n",
       "\n",
       "[880 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14fd2c2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dims: (880, 27)\n",
      "y_train dims: (880,)\n",
      "X_test dims: (378, 27)\n",
      "y_test dims: (378,)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"/home/marilin/xAutoML-Project1/fjodor/project-1/data/clean/train_clean.csv\", delimiter = \";\")\n",
    "test_df = pd.read_csv(\"/home/marilin/xAutoML-Project1/fjodor/project-1/data/clean/test_clean.csv\", delimiter = \";\")\n",
    "\n",
    "X_train = train_df.iloc[:,:27]\n",
    "y_train = train_df['compliance']\n",
    "\n",
    "# Training and validation data\n",
    "#X_train, X_val, y_train, y_val = train_test_split(train_df.iloc[:,:27], train_df['compliance_2020'], test_size=0.33, random_state=42)\n",
    "\n",
    "# Test data\n",
    "X_test = test_df.iloc[:,:27]\n",
    "y_test = test_df['compliance']\n",
    "\n",
    "print(f'X_train dims: {X_train.shape}')\n",
    "print(f'y_train dims: {y_train.shape}')\n",
    "\n",
    "#print(f'X_val dims: {X_val.shape}')\n",
    "#print(f'y_val dims: {y_val.shape}')\n",
    "\n",
    "print(f'X_test dims: {X_test.shape}')\n",
    "print(f'y_test dims: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a4cddf",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "A great resources for hyperparameter optimization using the `hyperopt`library can be found [here](https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce).\n",
    "\n",
    "Running the hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "530a6028",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"Gradient Boosting Classifier\",\n",
    "    \"LDA\",\n",
    "    \"Random Forest\",\n",
    "    \"AdaBoost\",\n",
    "    \"Logistic Regression\"\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    GradientBoostingClassifier(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    LogisticRegression() \n",
    "]\n",
    "\n",
    "search_spaces = [\n",
    "    \n",
    "    # GBC\n",
    "    {\n",
    "        'learning_rate': hp.choice('learning_rate', np.arange(0.001, 0.4, 0.001)), # default=0.1\n",
    "        'n_estimators': hp.choice('n_estimators', range(50, 250, 1)), # default=100\n",
    "        'subsample': hp.choice('subsample', np.arange(0.01, 1, 0.01)), # default=1.0\n",
    "        'max_depth': hp.choice('max_depth', range(1,20)), # default=3\n",
    "        'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2']), # default=None\n",
    "     #   'criterion': hp.choice('criterion', [\"friedman_mse\", \"squared_error\"]) # default=’friedman_mse’\n",
    "    },\n",
    "    \n",
    "    # LDA\n",
    "    {\n",
    "        'solver': hp.choice('solver', ['lsqr', 'eigen']), # default=’svd’\n",
    "        'shrinkage': hp.choice('shrinkage', np.arange(0.01, 1, 0.01)), # default=None # not supported with 'svd' solver\n",
    "        \n",
    "    },\n",
    "    \n",
    "    # RF\n",
    "    {\n",
    "        'n_estimators': hp.choice('n_estimators', range(50, 250, 1)), # default=100\n",
    "        'min_samples_split': hp.choice('min_samples_split', range(2, 7, 1)), # default=2\n",
    "        'max_features': hp.choice('max_features', ['sqrt', 'log2']), # default='sqrt'\n",
    "    },\n",
    "    \n",
    "    # AdaBoost\n",
    "    {\n",
    "        'learning_rate': hp.choice('learning_rate', np.arange(0.001, 0.4, 0.001)), # default=1.0\n",
    "        'n_estimators': hp.choice('n_estimators', range(50, 250, 1)), # default=100\n",
    "        \n",
    "    },\n",
    "    \n",
    "    # Logistic Regression\n",
    "    { \n",
    "        'penalty': hp.choice('penalty', ['l1', 'l2', 'elasticnet', 'none']), # default=’l2’\n",
    "        'l1_ratio': hp.choice('l1_ratio', np.arange(0.01, 1, 0.01)),\n",
    "        'C': hp.choice('C', np.arange(0.01, 1, 0.01)), # default = 1.0\n",
    "        'solver': 'saga' # # use solver = 'saga' for using different penalties\n",
    "    }   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec99caa2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the *Gradient Boosting Classifier* for estimation.\n",
      "new best:                                                                                                \n",
      "0.93                                                                                                     \n",
      "{'learning_rate': 0.382, 'max_depth': 8, 'max_features': 'log2', 'n_estimators': 69, 'subsample': 0.3}   \n",
      "new best:                                                                                                \n",
      "0.9738636363636364                                                                                       \n",
      "{'learning_rate': 0.08, 'max_depth': 14, 'max_features': 'auto', 'n_estimators': 79, 'subsample': 0.32}  \n",
      "new best:                                                                                                \n",
      "0.9740909090909091                                                                                       \n",
      "{'learning_rate': 0.177, 'max_depth': 7, 'max_features': 'auto', 'n_estimators': 59, 'subsample': 0.32}  \n",
      "new best:                                                                                                \n",
      "0.9772727272727272                                                                                       \n",
      "{'learning_rate': 0.078, 'max_depth': 14, 'max_features': 'auto', 'n_estimators': 221, 'subsample': 0.85}\n",
      "new best:                                                                                                \n",
      "0.9804545454545455                                                                                       \n",
      "{'learning_rate': 0.158, 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 146, 'subsample': 0.86} \n",
      "new best:                                                                                                \n",
      "0.9811363636363637                                                                                       \n",
      "{'learning_rate': 0.306, 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 208, 'subsample': 0.8200000000000001}\n",
      "new best:                                                                                                \n",
      "0.9818181818181817                                                                                       \n",
      "{'learning_rate': 0.162, 'max_depth': 5, 'max_features': 'auto', 'n_estimators': 146, 'subsample': 0.8200000000000001}\n",
      "new best:                                                                                                \n",
      "0.9834090909090909                                                                                       \n",
      "{'learning_rate': 0.23600000000000002, 'max_depth': 3, 'max_features': 'auto', 'n_estimators': 129, 'subsample': 0.74}\n",
      "new best:                                                                                                \n",
      "0.9834090909090911                                                                                       \n",
      "{'learning_rate': 0.383, 'max_depth': 6, 'max_features': 'auto', 'n_estimators': 183, 'subsample': 0.88} \n",
      "100%|███████████████████████████████| 100/100 [13:50<00:00,  8.31s/trial, best loss: -0.9834090909090911]\n",
      "\n",
      "Using the *LDA* for estimation.\n",
      "new best:                                                                                                \n",
      "0.8409090909090909                                                                                       \n",
      "{'shrinkage': 0.02, 'solver': 'eigen'}                                                                   \n",
      "new best:                                                                                                \n",
      "0.8515909090909091                                                                                       \n",
      "{'shrinkage': 0.64, 'solver': 'lsqr'}                                                                    \n",
      "new best:                                                                                                \n",
      "0.8572727272727272                                                                                       \n",
      "{'shrinkage': 0.76, 'solver': 'eigen'}                                                                   \n",
      "new best:                                                                                                \n",
      "0.8581818181818182                                                                                       \n",
      "{'shrinkage': 0.91, 'solver': 'eigen'}                                                                   \n",
      "new best:                                                                                                \n",
      "0.8643181818181818                                                                                       \n",
      "{'shrinkage': 0.99, 'solver': 'eigen'}                                                                   \n",
      "new best:                                                                                                \n",
      "0.8650000000000001                                                                                       \n",
      "{'shrinkage': 0.99, 'solver': 'eigen'}                                                                   \n",
      "new best:                                                                                                \n",
      "0.8654545454545454                                                                                       \n",
      "{'shrinkage': 0.98, 'solver': 'eigen'}                                                                   \n",
      "100%|███████████████████████████████| 100/100 [00:13<00:00,  7.23trial/s, best loss: -0.8654545454545454]\n",
      "\n",
      "Using the *Random Forest* for estimation.\n",
      "new best:                                                                                                \n",
      "0.9715909090909091                                                                                       \n",
      "{'max_features': 'log2', 'min_samples_split': 3, 'n_estimators': 150}                                    \n",
      "new best:                                                                                                \n",
      "0.9738636363636363                                                                                       \n",
      "{'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 233}                                    \n",
      "new best:                                                                                                \n",
      "0.9740909090909091                                                                                       \n",
      "{'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 221}                                    \n",
      "new best:                                                                                                \n",
      "0.9740909090909092                                                                                       \n",
      "{'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 83}                                     \n",
      "new best:                                                                                                \n",
      "0.975                                                                                                    \n",
      "{'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 167}                                    \n",
      "new best:                                                                                                \n",
      "0.9752272727272728                                                                                       \n",
      "{'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 102}                                    \n",
      "100%|███████████████████████████████| 100/100 [09:47<00:00,  5.87s/trial, best loss: -0.9752272727272728]\n",
      "\n",
      "Using the *AdaBoost* for estimation.\n",
      "new best:                                                                                                \n",
      "0.9793181818181818                                                                                       \n",
      "{'learning_rate': 0.386, 'n_estimators': 122}                                                            \n",
      "new best:                                                                                                \n",
      "0.9831818181818183                                                                                       \n",
      "{'learning_rate': 0.392, 'n_estimators': 113}                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best:                                                                                                \n",
      "0.9845454545454545                                                                                       \n",
      "{'learning_rate': 0.275, 'n_estimators': 163}                                                            \n",
      "100%|███████████████████████████████| 100/100 [10:33<00:00,  6.34s/trial, best loss: -0.9845454545454545]\n",
      "\n",
      "Using the *Logistic Regression* for estimation.\n",
      "new best:                                                                                                \n",
      "0.8654545454545455                                                                                       \n",
      "{'C': 0.26, 'l1_ratio': 0.2, 'penalty': 'elasticnet', 'solver': 'saga'}                                  \n",
      "new best:                                                                                                \n",
      "0.8672727272727272                                                                                       \n",
      "{'C': 0.73, 'l1_ratio': 0.9400000000000001, 'penalty': 'elasticnet', 'solver': 'saga'}                   \n",
      "new best:                                                                                                \n",
      "0.8675000000000002                                                                                       \n",
      "{'C': 0.9500000000000001, 'l1_ratio': 0.88, 'penalty': 'none', 'solver': 'saga'}                         \n",
      "new best:                                                                                                \n",
      "0.8681818181818182                                                                                       \n",
      "{'C': 0.5800000000000001, 'l1_ratio': 0.88, 'penalty': 'l2', 'solver': 'saga'}                           \n",
      "new best:                                                                                                \n",
      "0.8684090909090909                                                                                       \n",
      "{'C': 0.8200000000000001, 'l1_ratio': 0.74, 'penalty': 'l2', 'solver': 'saga'}                           \n",
      "new best:                                                                                                \n",
      "0.8699999999999999                                                                                       \n",
      "{'C': 0.75, 'l1_ratio': 0.42000000000000004, 'penalty': 'l2', 'solver': 'saga'}                          \n",
      "100%|███████████████████████████████| 100/100 [01:18<00:00,  1.28trial/s, best loss: -0.8699999999999999]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dataframe for results collection\n",
    "model_results = pd.DataFrame(columns = ['classifier', 'best_cv_score', 'runtime_hpo', 'best_params', 'test_acc'])\n",
    "model_results.head()\n",
    "\n",
    "# Run the HPO and get the best params for each classifier\n",
    "for i in range(len(classifiers)):\n",
    "    \n",
    "    print()\n",
    "    print(f'Using the *{names[i]}* for estimation.')\n",
    "    \n",
    "    def hyperopt_cv_score(params):\n",
    "        cv = RepeatedKFold(n_splits=5, n_repeats=5)\n",
    "        model = classifiers[i].set_params(**params) # use the classifier from the list\n",
    "        return cross_val_score(model, \n",
    "                               X_train, y_train, \n",
    "                               cv = cv,  \n",
    "                               scoring = 'accuracy',\n",
    "                               error_score='raise').mean() #np.mean(gbc.predict(X_test) == y_test)\n",
    "\n",
    "    def f(params):\n",
    "        global best_cv_score\n",
    "        global best_params\n",
    "        global best_time\n",
    "        cv_score = hyperopt_cv_score(params)\n",
    "        if cv_score > best_cv_score:\n",
    "            best_cv_score = cv_score # what is the best score?\n",
    "            best_params = params # what are the best params?\n",
    "            best_time = round(time.time() - start_time,4) # track how much time it took to find the best params\n",
    "            print('new best:', best_cv_score, best_params)\n",
    "        return {'loss': -cv_score, # see the comment below\n",
    "                'status': STATUS_OK}\n",
    "        # Comment regarding 'negative accuracy' (from the referenced source):\n",
    "        ## Since we are trying to maximize the accuracy (acc in the code above), \n",
    "        ## we must negate this value for hyperopt, since hyperopt only knows how to minimize a function. \n",
    "        ## Minimizing a function f is the same as maximizing the negative of f.\n",
    "\n",
    "    best_cv_score = 0\n",
    "    best_params = None\n",
    "    best_time = 0\n",
    "    trials = Trials() # store info at each step\n",
    "    \n",
    "    # Start running the algorithm and track time\n",
    "    start_time = time.time()\n",
    "    best = fmin(f, \n",
    "                search_spaces[i], # use the search space associated with the classifier\n",
    "                algo = tpe.suggest, \n",
    "                max_evals = 100, # how many evaluations?\n",
    "                trials = trials)\n",
    "    \n",
    "    # save trials\n",
    "    \n",
    "    \n",
    "    # Compute the accuracy score on the best model of the classifier\n",
    "    m = classifiers[i].set_params(**best_params).fit(X_train, y_train)\n",
    "    score_test = m.score(X_test, y_test)\n",
    "    \n",
    "    \n",
    "    # Append the best results to the df\n",
    "    model_results = model_results.append({'classifier': names[i], \n",
    "                             'best_cv_score': best_cv_score, \n",
    "                             'runtime_hpo': best_time, \n",
    "                             'best_params': best_params,\n",
    "                             'test_acc': score_test}, ignore_index = True)\n",
    "\n",
    "# Sort the model results by test accuracy score and then Runtime\n",
    "model_results = model_results.sort_values(['test_acc', 'runtime_hpo'], ascending = [0,1]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c44771",
   "metadata": {},
   "source": [
    "Let's look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00b6a93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>best_cv_score</th>\n",
       "      <th>runtime_hpo</th>\n",
       "      <th>best_params</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.983409</td>\n",
       "      <td>530.1861</td>\n",
       "      <td>{'learning_rate': 0.383, 'max_depth': 6, 'max_...</td>\n",
       "      <td>0.986772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.984545</td>\n",
       "      <td>43.6207</td>\n",
       "      <td>{'learning_rate': 0.275, 'n_estimators': 163}</td>\n",
       "      <td>0.981481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.975227</td>\n",
       "      <td>294.8328</td>\n",
       "      <td>{'max_features': 'sqrt', 'min_samples_split': ...</td>\n",
       "      <td>0.965608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>74.9819</td>\n",
       "      <td>{'C': 0.75, 'l1_ratio': 0.42000000000000004, '...</td>\n",
       "      <td>0.891534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LDA</td>\n",
       "      <td>0.865455</td>\n",
       "      <td>9.9723</td>\n",
       "      <td>{'shrinkage': 0.98, 'solver': 'eigen'}</td>\n",
       "      <td>0.865079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     classifier  best_cv_score  runtime_hpo  \\\n",
       "0  Gradient Boosting Classifier       0.983409     530.1861   \n",
       "1                      AdaBoost       0.984545      43.6207   \n",
       "2                 Random Forest       0.975227     294.8328   \n",
       "3           Logistic Regression       0.870000      74.9819   \n",
       "4                           LDA       0.865455       9.9723   \n",
       "\n",
       "                                         best_params  test_acc  \n",
       "0  {'learning_rate': 0.383, 'max_depth': 6, 'max_...  0.986772  \n",
       "1      {'learning_rate': 0.275, 'n_estimators': 163}  0.981481  \n",
       "2  {'max_features': 'sqrt', 'min_samples_split': ...  0.965608  \n",
       "3  {'C': 0.75, 'l1_ratio': 0.42000000000000004, '...  0.891534  \n",
       "4             {'shrinkage': 0.98, 'solver': 'eigen'}  0.865079  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910a2f4",
   "metadata": {},
   "source": [
    "Let's plot the model test set accuracy against runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc5b7b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = [5, 5])\n",
    "\n",
    "for i in range(len(model_results)):\n",
    "    # Add classifier values\n",
    "    plt.scatter(model_results['runtime_hpo'][i], \n",
    "            model_results['test_acc'][i], \n",
    "            label = model_results['classifier'][i],\n",
    "               marker = 'x',\n",
    "               s = 60)\n",
    "    # Add a dashed lined marking the best accuracy threshold for classifier\n",
    "    plt.hlines(y = model_results['test_acc'][i], \n",
    "               xmin=0, \n",
    "               xmax = np.max(model_results['runtime_hpo'])+1,\n",
    "              linestyles = 'dashed', alpha = 0.3, color = 'grey')\n",
    "    # Add classifier name\n",
    "    plt.annotate(model_results['classifier'][i], \n",
    "                 (model_results['runtime_hpo'][i]+1, \n",
    "                  model_results['test_acc'][i]),\n",
    "                )\n",
    "plt.ylim(bottom = 0.70, top = 1.01)\n",
    "plt.xlabel('Runtime (s)')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.title('Runtime until the best parameters were found')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ad627",
   "metadata": {},
   "source": [
    "Let's plot the accuracy values over each iteration with the baseline accuracy as the reference point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6502ec",
   "metadata": {},
   "source": [
    "Let's plot the values of parameters and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b884ec3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot for parameters\n",
    "## Tutorial: https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce\n",
    "#parameters = ['learning_rate', 'n_estimators',\n",
    "#              'subsample', 'max_depth', \n",
    "#              'max_features', 'criterion']\n",
    "\n",
    "#f, axes = plt.subplots(nrows=2, ncols=3, figsize=(10,5))\n",
    "#f.tight_layout()\n",
    "#cmap = plt.cm.jet\n",
    "#for i, val in enumerate(parameters):\n",
    "#    # print (i, val)\n",
    "#    # Getting the hyperparameter and loss values with list comprehension\n",
    "#    xs = np.array([t['misc']['vals'][val] for t in trials.trials]).ravel()\n",
    "#    ys = [-t['result']['loss'] for t in trials.trials]\n",
    "#    # Making the necessary sorting\n",
    "#    xs, ys =  zip(*sorted(zip(xs, ys)))\n",
    "#    ys = np.array(ys)\n",
    "#    # Plot\n",
    "#    axes[int(i/3),int(i%3)].scatter(xs, ys, \n",
    "#                          s=20, \n",
    "#                          linewidth=0.01, \n",
    "#                         # alpha= 0.5,\n",
    "#                          color=cmap(float(i)/len(parameters))\n",
    "#                                   )\n",
    "#    axes[int(i/3),int(i%3)].set_title(val)\n",
    "#    axes[int(i/3),int(i%3)].set_ylim([0.2,1.0])\n",
    "#    \n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2dd412",
   "metadata": {},
   "source": [
    "## Experimenting with `hyperopt-sklearn`\n",
    "Above, we provided the definition for the search space. Hoqever, `hyperopt` can also function without explicitly specifying the search space - enter `hyperopt-sklearn`, a presently developed library (see [here](https://github.com/hyperopt/hyperopt-sklearn) that provides more automation for hyperparameter optimization. \n",
    "\n",
    "Below we will run the classifier used above without specified search space. Of note, this might mean that some models will not compute or converge, since some hyperparameter combinations do not work together. In those cases, we just skip the hyperparameter combination and proceed.\n",
    "\n",
    "The advantages is that we can reduce manual work in specifying hyperparameter space for each classifier. The main disadvantage is that some of the pereviously-used functions are not easily programmable here, i.e., the `hypropt-sklearn` is somewhat less flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feee3256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/hyperopt/hyperopt-sklearn\n",
      "  Cloning https://github.com/hyperopt/hyperopt-sklearn to /tmp/pip-req-build-_d3a9kw6\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/hyperopt/hyperopt-sklearn /tmp/pip-req-build-_d3a9kw6\n",
      "  Resolved https://github.com/hyperopt/hyperopt-sklearn to commit 1cce406bedbc9947d19792cd79975d15e391dded\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.7.1 in /home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages (from hpsklearn==1.0.3) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages (from hpsklearn==1.0.3) (1.23.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in /home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages (from hpsklearn==1.0.3) (1.1.3)\n",
      "Requirement already satisfied: hyperopt>=0.2.6 in /home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages (from hpsklearn==1.0.3) (0.2.7)\n",
      "Requirement already satisfied: py4j in /home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (0.10.9.7)\n",
      "Requirement already satisfied: networkx>=2.2 in /home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (2.8.8)\n",
      "Requirement already satisfied: future in /home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (0.18.2)\n",
      "Requirement already satisfied: cloudpickle in /home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (2.2.0)\n",
      "Requirement already satisfied: tqdm in /home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (4.64.1)\n",
      "Requirement already satisfied: six in /home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages (from scikit-learn>=1.0->hpsklearn==1.0.3) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages (from scikit-learn>=1.0->hpsklearn==1.0.3) (2.2.0)\n",
      "Building wheels for collected packages: hpsklearn\n",
      "  Building wheel for hpsklearn (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hpsklearn: filename=hpsklearn-1.0.3-py3-none-any.whl size=135026 sha256=12f394e75d67c9cc6781744817091a249e7a04abeb9c1a3686b2d5b10619cd75\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ewg30pkg/wheels/a5/65/a4/0858cf98f1400a9bfa5af96f757b17ccbf399eca01d71212e0\n",
      "Successfully built hpsklearn\n",
      "Installing collected packages: hpsklearn\n",
      "Successfully installed hpsklearn-1.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/hyperopt/hyperopt-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c79248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/hyperopt/hyperopt-sklearn\n",
    "from hpsklearn import HyperoptEstimator, gradient_boosting_classifier, ada_boost_classifier, random_forest_classifier, logistic_regression, linear_discriminant_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58009fa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the *Gradient Boosting Classifier* for estimation.\n",
      "100%|████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.07trial/s, best loss: 0.1875]\n",
      " 50%|█████████████████████████████▌                             | 1/2 [15:18:48<?, ?trial/s, best loss=?]\n",
      "\n",
      "Using the *LDA* for estimation.\n",
      "  0%|                                                              | 0/1 [00:08<?, ?trial/s, best loss=?]\n",
      "Model with params not computed\n",
      "\n",
      "Using the *Random Forest* for estimation.\n",
      "  0%|                                                              | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                              | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/marilin/miniconda3/envs/automl_env/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "\n",
      "  File \"/home/marilin/miniconda3/envs/automl_env/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "\n",
      "  File \"/home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages/hpsklearn/estimator/_cost_fn.py\", line 199, in _cost_fn\n",
      "    learner.fit(XEXfit, yfit)\n",
      "\n",
      "  File \"/home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 465, in fit\n",
      "    trees = [\n",
      "\n",
      "  File \"/home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 466, in <listcomp>\n",
      "    self._make_estimator(append=False, random_state=random_state)\n",
      "\n",
      "  File \"/home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages/sklearn/ensemble/_base.py\", line 186, in _make_estimator\n",
      "    _set_random_states(estimator, random_state)\n",
      "\n",
      "  File \"/home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages/sklearn/ensemble/_base.py\", line 81, in _set_random_states\n",
      "    for key in sorted(estimator.get_params(deep=True)):\n",
      "\n",
      "  File \"/home/marilin/miniconda3/envs/automl_env/lib/python3.9/site-packages/sklearn/base.py\", line 215, in get_params\n",
      "    out[key] = value\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model with params not computed\n",
      "\n",
      "Using the *AdaBoost* for estimation.\n",
      "  0%|                                                              | 0/1 [00:00<?, ?trial/s, best loss=?]\n",
      "Model with params not computed\n",
      "\n",
      "Using the *Logistic Regression* for estimation.\n",
      "  0%|                                                              | 0/1 [00:00<?, ?trial/s, best loss=?]\n",
      "Model with params not computed\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize a dataframe\n",
    "model_results_auto = pd.DataFrame(columns = ['classifier', 'test_acc', 'best_params'])\n",
    "\n",
    "# 2. Classifier labels\n",
    "names_auto = [\n",
    "    \"Gradient Boosting Classifier\",\n",
    "    \"LDA\",\n",
    "    \"Random Forest\",\n",
    "    \"AdaBoost\",\n",
    "    \"Logistic Regression\"\n",
    "]\n",
    "\n",
    "# 3. Classifier classes\n",
    "classifiers_auto = [\n",
    "    gradient_boosting_classifier(\"gbc\"),\n",
    "    linear_discriminant_analysis('lda'),\n",
    "    random_forest_classifier('rf'), \n",
    "    ada_boost_classifier('ada'), \n",
    "    logistic_regression('logreg')\n",
    "]\n",
    "\n",
    "# 4. Iterate over all classifiers\n",
    "for i in range(len(classifiers_auto)):\n",
    "    print()\n",
    "    print(f'Using the *{names_auto[i]}* for estimation.')\n",
    "    \n",
    "    # Objective function\n",
    "    estim = HyperoptEstimator(classifier = classifiers_auto[i],\n",
    "                                    #  preprocessing=[],\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=100)\n",
    "    try:\n",
    "        # Fit the model\n",
    "        estim.fit(X_train, y_train)\n",
    "        \n",
    "        # Add the results to df\n",
    "        model_results_auto = model_results_auto.append(\n",
    "            {'classifier': names_auto[i], \n",
    "            # 'runtime_hpo': best_time,\n",
    "             'test_acc': estim.score(X_test, y_test),\n",
    "             'best_params': estim.best_model()\n",
    "             }, \n",
    "            ignore_index = True)\n",
    "    except:\n",
    "        print('Model with params not computed')\n",
    "        # Append the name but assign None-values to fields\n",
    "        model_results_auto = model_results_auto.append(\n",
    "            {'classifier': names_auto[i], \n",
    "            # 'runtime_hpo': best_time,\n",
    "             'test_acc': None,\n",
    "             'best_params': None\n",
    "             }, \n",
    "            ignore_index = True)\n",
    "\n",
    "model_results_auto = model_results_auto.sort_values(['test_acc'], ascending = False).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34a7ec",
   "metadata": {},
   "source": [
    "Look at the results table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "765af402",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.798942</td>\n",
       "      <td>{'learner': ([DecisionTreeRegressor(criterion=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     classifier  test_acc  \\\n",
       "0  Gradient Boosting Classifier  0.798942   \n",
       "1                           LDA       NaN   \n",
       "2                 Random Forest       NaN   \n",
       "3                      AdaBoost       NaN   \n",
       "4           Logistic Regression       NaN   \n",
       "\n",
       "                                         best_params  \n",
       "0  {'learner': ([DecisionTreeRegressor(criterion=...  \n",
       "1                                               None  \n",
       "2                                               None  \n",
       "3                                               None  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results_auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75703c96",
   "metadata": {},
   "source": [
    "# Significance testing \n",
    "In the below section, we will test whether the differences between the models are statistically significant. For that, we will use pairwise McNemar test to compare each model with each other. P-values have the continuity correction.\n",
    "\n",
    "We first start by defining the helper function for the pairwise McNemar test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8327e546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# McNemar test for practice sessions/homeworks into a function\n",
    "def mcnemar_test(ground_truth, preds1, preds2):\n",
    "    \"\"\"Get the McNemar test results for comparison between two models\n",
    "    Args: \n",
    "        ground_truth (np.array): the ground truth, an np.array (usually y_test)\n",
    "        preds1 (np.array): predictions from Model 1\n",
    "        preds2 (np.array): predictions from Model 2\n",
    "    Returns:\n",
    "        statistic (float): McNemar statistic\n",
    "        pvalue (float): p-value of the significance test\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Create the prediction data set\n",
    "    mc_data = pd.DataFrame()\n",
    "    mc_data[\"ground_truth\"] = y_test.tolist()\n",
    "    mc_data[\"y_pred1\"] = preds1.tolist()\n",
    "    mc_data[\"y_pred2\"] = preds2.tolist()\n",
    "    # mc_data.head(5)\n",
    "\n",
    "    ## Create the contingency table\n",
    "    mc_data.loc[mc_data['ground_truth']==mc_data['y_pred1'],'correct_pred1']=\"Yes\"\n",
    "    mc_data.loc[mc_data['ground_truth']!=mc_data['y_pred1'],'correct_pred1']=\"No\"\n",
    "    mc_data.loc[mc_data['ground_truth']==mc_data['y_pred2'],'correct_pred2']=\"Yes\"\n",
    "    mc_data.loc[mc_data['ground_truth']!=mc_data['y_pred2'],'correct_pred2']=\"No\"\n",
    "\n",
    "    contingency_table_df=pd.DataFrame(data={\"nr_correct_pred1\":[\"Yes/Yes\",\"No/Yes\"], \n",
    "                                            \"nr_incorrect_pred1\":[\"Yes/No\",\"No/No\"]}, \n",
    "                                      index=[\"nr_correct_pred2\",\"nr_incorrect_pred2\"])\n",
    "\n",
    "    ## Add the numbers to contingency tables\n",
    "    nr_corr_pred2_corr_pred1=0\n",
    "    nr_corr_pred2_incorr_pred1=0\n",
    "    nr_incorr_pred2_corr_pred1=0\n",
    "    nr_incorr_pred2_incorr_pred1=0\n",
    "    for index, row in mc_data.iterrows():\n",
    "        if  row['correct_pred2']== \"Yes\" and  row['correct_pred1']==\"Yes\":\n",
    "            nr_corr_pred2_corr_pred1+=1\n",
    "        elif row['correct_pred2']== \"Yes\" and  row['correct_pred1']==\"No\":\n",
    "            nr_corr_pred2_incorr_pred1+=1\n",
    "        elif row['correct_pred2']== \"No\" and  row['correct_pred1']==\"Yes\":\n",
    "            nr_incorr_pred2_corr_pred1+=1\n",
    "        elif row['correct_pred2']== \"No\" and  row['correct_pred1']==\"No\":\n",
    "            nr_incorr_pred2_incorr_pred1+=1\n",
    "\n",
    "    contingency_table_df.iloc[0,0]=nr_corr_pred2_corr_pred1\n",
    "    contingency_table_df.iloc[0,1]=nr_corr_pred2_incorr_pred1\n",
    "    contingency_table_df.iloc[1,0]=nr_incorr_pred2_corr_pred1\n",
    "    contingency_table_df.iloc[1,1]=nr_incorr_pred2_incorr_pred1\n",
    "\n",
    "    # Calculate The McNemar test statistic and p-value\n",
    "    mc_results = mcnemar(contingency_table_df, exact=False, correction=True)\n",
    "    return mc_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0e9c7",
   "metadata": {},
   "source": [
    "Next, we iterate over all models and compare their performance against all other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f027fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the classifier\n",
    "df_sig = pd.DataFrame(columns = ['model1', 'model2', 'mc_stat', 'pvalue_adj'])\n",
    "\n",
    "## Ground truth\n",
    "ground_truth = y_test\n",
    "\n",
    "comparisons = []\n",
    "\n",
    "for i in range(len(model_results)):\n",
    "    for j in range(len(model_results)-1):\n",
    "        # Names of models\n",
    "        name1 = model_results['classifier'][i]\n",
    "        name2 = model_results['classifier'][j+1]\n",
    "        \n",
    "        if name1 == name2: # no point in comparing the model with self\n",
    "            pass\n",
    "        else:\n",
    "            comparison = list(set((name1, name2))) # no point in duplicating comparisons\n",
    "        \n",
    "            if comparison not in comparisons:\n",
    "                comparisons.append(comparison)\n",
    "                \n",
    "                # Model 1\n",
    "                idx1 = int(np.where(np.array(names) == name1)[0])\n",
    "                classifier1 = classifiers[idx1]\n",
    "                best_params1 = model_results['best_params'][i]\n",
    "                best_model1 = classifier1.set_params(**best_params1).fit(X_train, y_train)\n",
    "                preds1 = best_model1.predict(X_test)\n",
    "\n",
    "                ## Model 2\n",
    "                idx2 = int(np.where(np.array(names) == model_results['classifier'][j+1])[0])\n",
    "                classifier2 = classifiers[idx2]\n",
    "                best_params2 = model_results['best_params'][j+1]\n",
    "                best_model2 = classifier2.set_params(**best_params2).fit(X_train, y_train)\n",
    "                preds2 = best_model2.predict(X_test)\n",
    "\n",
    "                # McNemar test\n",
    "                mc = mcnemar_test(ground_truth, preds1, preds2)\n",
    "                pvalue, statistic = mc.pvalue, mc.statistic\n",
    "                \n",
    "                # Append to the dataframe\n",
    "                df_sig = df_sig.append({'model1': name1, \n",
    "                               'model2': name2, \n",
    "                               'mc_stat': statistic, \n",
    "                               'pvalue_adj': pvalue}, ignore_index = True)\n",
    "            else:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304d6dc",
   "metadata": {},
   "source": [
    "Let's look at the pairwise differences table. Just a reminder that p-values have the continuity correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "26ee411f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>mc_stat</th>\n",
       "      <th>pvalue_adj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>140.007042</td>\n",
       "      <td>2.652612e-32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>137.007194</td>\n",
       "      <td>1.201490e-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>LDA</td>\n",
       "      <td>135.007299</td>\n",
       "      <td>3.289573e-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>131.007519</td>\n",
       "      <td>2.466702e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>134.007353</td>\n",
       "      <td>5.443351e-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>LDA</td>\n",
       "      <td>135.007299</td>\n",
       "      <td>3.289573e-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>130.007576</td>\n",
       "      <td>4.082167e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>LDA</td>\n",
       "      <td>129.007634</td>\n",
       "      <td>6.755805e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>129.007634</td>\n",
       "      <td>6.755805e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LDA</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>130.007576</td>\n",
       "      <td>4.082167e-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model1                        model2     mc_stat  \\\n",
       "0                      AdaBoost                 Random Forest  140.007042   \n",
       "1                      AdaBoost  Gradient Boosting Classifier  137.007194   \n",
       "2                      AdaBoost                           LDA  135.007299   \n",
       "3                      AdaBoost           Logistic Regression  131.007519   \n",
       "4                 Random Forest  Gradient Boosting Classifier  134.007353   \n",
       "5                 Random Forest                           LDA  135.007299   \n",
       "6                 Random Forest           Logistic Regression  130.007576   \n",
       "7  Gradient Boosting Classifier                           LDA  129.007634   \n",
       "8  Gradient Boosting Classifier           Logistic Regression  129.007634   \n",
       "9                           LDA           Logistic Regression  130.007576   \n",
       "\n",
       "     pvalue_adj  \n",
       "0  2.652612e-32  \n",
       "1  1.201490e-31  \n",
       "2  3.289573e-31  \n",
       "3  2.466702e-30  \n",
       "4  5.443351e-31  \n",
       "5  3.289573e-31  \n",
       "6  4.082167e-30  \n",
       "7  6.755805e-30  \n",
       "8  6.755805e-30  \n",
       "9  4.082167e-30  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d84d92",
   "metadata": {},
   "source": [
    "<font color= 'red'> Something's probably not right here. RF and GBC have the same test accuracy yet they are different here... </font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
