{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581c6cab",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189bd59a",
   "metadata": {},
   "source": [
    "After evaluating the performance of different modelling frameworks with baseline hyperparameters, the performance of the Gradiendt Boosting Classifier showed the highest accuracy score. Hence, we are proceeding with the Gradient Boosting Classifier (GBC), or the `sklearn` class `GradientBoostingClassifier()`.\n",
    "\n",
    "### Gradient Boosting Classifier\n",
    "#### What is Gradient Boosting?\n",
    "Before we start examining the hyperparameters of the Gradient Boosting Classifier (`GradientBoostingClassifier()`), it would be a good idea to recap on what this classifier is. So what is GBC? According to [this source](https://medium.com/analytics-vidhya/introduction-to-gradient-boosting-classification-da4e81f54d3):\n",
    "\n",
    ">Gradient Boosting is the grouping of gradient descent and boosting. In gradient boosting, each new model minimizes the loss function from its predecessor using the Gradient Descent Method. This procedure continues until a more optimal estimate of the target variable has been achieved.\n",
    "\n",
    "GBC, therefore, is a sequential ensemble model - meaning that in order to achieve great accuracy, a number of models need to run. Importantly, this cannot be done in parallel (hence, 'sequential'). This is important to note, as GBCs can get computationally costly, especially in a large hyperparameter space.\n",
    "\n",
    "#### Hyperparameters of GBC (and `GradientBoostingClassifier()`)\n",
    "The Hyperparameters of GBC are quite neatly covered in the [previously-used source](https://medium.com/analytics-vidhya/introduction-to-gradient-boosting-classification-da4e81f54d3). Here is a brief overview of most important hyperparameters:\n",
    "\n",
    "- **Learning rate**: by using gradient descent, this hyperparameter affects the speed of algorithm convergence. There is a tradeoff between the value size with smaller values minimizing overfitting (but taking more time until convergence) and larger values, while potentially not reaching an optimal fit, lead to a faster convergence.\n",
    "\n",
    "- **Number of trees**: how many trees are optimal to minimize the loss function? \n",
    "\n",
    "- **Depth of trees**: the number of splitsin a tree.\n",
    "\n",
    "- **Subsampling**: what is the proportion of trainig data used for training a tree? \n",
    "\n",
    "All this briefly summarized, let's start the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518c6e7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065c3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "#!pip install hyperopt\n",
    "#!pip install git+https://github.com/hyperopt/hyperopt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69141ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "## General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "## Model training and evaluation\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Classifiers\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "## Hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import hyperopt.pyll.stochastic\n",
    "\n",
    "## Misc\n",
    "from statsmodels.stats.contingency_tables import mcnemar # McNemar test\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# About FMIN: https://github.com/hyperopt/hyperopt/wiki/FMin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29786a20",
   "metadata": {},
   "source": [
    "## Import the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69defb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aluminium;ammonium;boron;chloride;coli-like-bacteria-colilert;coli-like-bacteria;colony-count-at-22-c;color-pt-co-unit;color-pt/co-scale;electrical-conductivity;enterococci;escherichia-coli-colilert;escherichia-coli;fluoride;iron;manganese;nitrate;nitrite;odour-dilution-level;oxidability;smell-ball-units;sodium;sulphate;taste-ball-units;taste-dilution-degree;turbidity-ntu;ph ;compliance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0;0.05;0.2;23.0;0.0;0.0;5.0;1.8;2.1;474.0;0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0;0.05;0.2;23.0;0.0;0.0;0.0;5.0;0.0;465.0;0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0;0.05;0.2;23.0;0.0;0.0;0.0;5.0;0.0;448.0;0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0;0.09;0.641;23.0;0.0;0.0;15.0;5.0;4.0;978.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0;0.06;0.2;23.0;0.0;0.0;1.0;6.6;2.1;446.0;0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>5.0;0.05;0.2;23.0;0.0;0.0;3.0;5.0;3.0;622.0;0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>5.0;0.05;0.2;23.0;0.0;0.0;10.0;5.0;5.0;500.0;0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>5.0;0.27;0.274;14.0;0.0;0.0;66.0;5.0;6.0;596.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>5.0;0.05;0.2;23.0;0.0;0.0;1.0;2.7;2.1;425.0;0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>5.0;0.05;0.2;23.0;0.0;0.0;18.0;5.0;3.0;603.0;0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>880 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aluminium;ammonium;boron;chloride;coli-like-bacteria-colilert;coli-like-bacteria;colony-count-at-22-c;color-pt-co-unit;color-pt/co-scale;electrical-conductivity;enterococci;escherichia-coli-colilert;escherichia-coli;fluoride;iron;manganese;nitrate;nitrite;odour-dilution-level;oxidability;smell-ball-units;sodium;sulphate;taste-ball-units;taste-dilution-degree;turbidity-ntu;ph ;compliance\n",
       "0    5.0;0.05;0.2;23.0;0.0;0.0;5.0;1.8;2.1;474.0;0....                                                                                                                                                                                                                                                                                                                                                   \n",
       "1    5.0;0.05;0.2;23.0;0.0;0.0;0.0;5.0;0.0;465.0;0....                                                                                                                                                                                                                                                                                                                                                   \n",
       "2    5.0;0.05;0.2;23.0;0.0;0.0;0.0;5.0;0.0;448.0;0....                                                                                                                                                                                                                                                                                                                                                   \n",
       "3    5.0;0.09;0.641;23.0;0.0;0.0;15.0;5.0;4.0;978.0...                                                                                                                                                                                                                                                                                                                                                   \n",
       "4    5.0;0.06;0.2;23.0;0.0;0.0;1.0;6.6;2.1;446.0;0....                                                                                                                                                                                                                                                                                                                                                   \n",
       "..                                                 ...                                                                                                                                                                                                                                                                                                                                                   \n",
       "875  5.0;0.05;0.2;23.0;0.0;0.0;3.0;5.0;3.0;622.0;0....                                                                                                                                                                                                                                                                                                                                                   \n",
       "876  5.0;0.05;0.2;23.0;0.0;0.0;10.0;5.0;5.0;500.0;0...                                                                                                                                                                                                                                                                                                                                                   \n",
       "877  5.0;0.27;0.274;14.0;0.0;0.0;66.0;5.0;6.0;596.0...                                                                                                                                                                                                                                                                                                                                                   \n",
       "878  5.0;0.05;0.2;23.0;0.0;0.0;1.0;2.7;2.1;425.0;0....                                                                                                                                                                                                                                                                                                                                                   \n",
       "879  5.0;0.05;0.2;23.0;0.0;0.0;18.0;5.0;3.0;603.0;0...                                                                                                                                                                                                                                                                                                                                                   \n",
       "\n",
       "[880 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14fd2c2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dims: (880, 27)\n",
      "y_train dims: (880,)\n",
      "X_test dims: (378, 27)\n",
      "y_test dims: (378,)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"/home/marilin/xAutoML-Project1/fjodor/project-1/data/clean/train_clean.csv\", delimiter = \";\")\n",
    "test_df = pd.read_csv(\"/home/marilin/xAutoML-Project1/fjodor/project-1/data/clean/test_clean.csv\", delimiter = \";\")\n",
    "\n",
    "X_train = train_df.iloc[:,:27]\n",
    "y_train = train_df['compliance']\n",
    "\n",
    "# Training and validation data\n",
    "#X_train, X_val, y_train, y_val = train_test_split(train_df.iloc[:,:27], train_df['compliance_2020'], test_size=0.33, random_state=42)\n",
    "\n",
    "# Test data\n",
    "X_test = test_df.iloc[:,:27]\n",
    "y_test = test_df['compliance']\n",
    "\n",
    "print(f'X_train dims: {X_train.shape}')\n",
    "print(f'y_train dims: {y_train.shape}')\n",
    "\n",
    "#print(f'X_val dims: {X_val.shape}')\n",
    "#print(f'y_val dims: {y_val.shape}')\n",
    "\n",
    "print(f'X_test dims: {X_test.shape}')\n",
    "print(f'y_test dims: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a4cddf",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "A great resources for hyperparameter optimization using the `hyperopt`library can be found [here](https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce).\n",
    "\n",
    "Running the hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "530a6028",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"Gradient Boosting Classifier\",\n",
    "    \"LDA\",\n",
    "    \"Random Forest\",\n",
    "    \"AdaBoost\",\n",
    "    \"Logistic Regression\"\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    GradientBoostingClassifier(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    LogisticRegression() \n",
    "]\n",
    "\n",
    "search_spaces = [\n",
    "    \n",
    "    # GBC\n",
    "    {\n",
    "        'learning_rate': hp.choice('learning_rate', np.arange(0.001, 0.4, 0.001)), # default=0.1\n",
    "        'n_estimators': hp.choice('n_estimators', range(50, 250, 1)), # default=100\n",
    "        'subsample': hp.choice('subsample', np.arange(0.01, 1, 0.01)), # default=1.0\n",
    "        'max_depth': hp.choice('max_depth', range(1,20)), # default=3\n",
    "        'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2']), # default=None\n",
    "     #   'criterion': hp.choice('criterion', [\"friedman_mse\", \"squared_error\"]) # default=’friedman_mse’\n",
    "    },\n",
    "    \n",
    "    # LDA\n",
    "    {\n",
    "        'solver': hp.choice('solver', ['lsqr', 'eigen']), # default=’svd’\n",
    "        'shrinkage': hp.choice('shrinkage', np.arange(0.01, 1, 0.01)), # default=None # not supported with 'svd' solver\n",
    "        \n",
    "    },\n",
    "    \n",
    "    # RF\n",
    "    {\n",
    "        'n_estimators': hp.choice('n_estimators', range(50, 250, 1)), # default=100\n",
    "        'min_samples_split': hp.choice('min_samples_split', range(2, 7, 1)), # default=2\n",
    "        'max_features': hp.choice('max_features', ['sqrt', 'log2']), # default='sqrt'\n",
    "    },\n",
    "    \n",
    "    # AdaBoost\n",
    "    {\n",
    "        'learning_rate': hp.choice('learning_rate', np.arange(0.001, 0.4, 0.001)), # default=1.0\n",
    "        'n_estimators': hp.choice('n_estimators', range(50, 250, 1)), # default=100\n",
    "        \n",
    "    },\n",
    "    \n",
    "    # Logistic Regression\n",
    "    { \n",
    "        'penalty': hp.choice('penalty', ['l1', 'l2', 'elasticnet', 'none']), # default=’l2’\n",
    "        'l1_ratio': hp.choice('l1_ratio', np.arange(0.01, 1, 0.01)),\n",
    "        'C': hp.choice('C', np.arange(0.01, 1, 0.01)), # default = 1.0\n",
    "        'solver': 'saga' # # use solver = 'saga' for using different penalties\n",
    "    }   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99caa2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the *Gradient Boosting Classifier* for estimation.\n",
      "new best:                                                                                                \n",
      "0.93                                                                                                     \n",
      "{'learning_rate': 0.382, 'max_depth': 8, 'max_features': 'log2', 'n_estimators': 69, 'subsample': 0.3}   \n",
      "new best:                                                                                                \n",
      "0.9738636363636364                                                                                       \n",
      "{'learning_rate': 0.08, 'max_depth': 14, 'max_features': 'auto', 'n_estimators': 79, 'subsample': 0.32}  \n",
      "new best:                                                                                                \n",
      "0.9740909090909091                                                                                       \n",
      "{'learning_rate': 0.177, 'max_depth': 7, 'max_features': 'auto', 'n_estimators': 59, 'subsample': 0.32}  \n",
      "new best:                                                                                                \n",
      "0.9772727272727272                                                                                       \n",
      "{'learning_rate': 0.078, 'max_depth': 14, 'max_features': 'auto', 'n_estimators': 221, 'subsample': 0.85}\n",
      "new best:                                                                                                \n",
      "0.9804545454545455                                                                                       \n",
      "{'learning_rate': 0.158, 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 146, 'subsample': 0.86} \n",
      "new best:                                                                                                \n",
      "0.9811363636363637                                                                                       \n",
      "{'learning_rate': 0.306, 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 208, 'subsample': 0.8200000000000001}\n",
      "new best:                                                                                                \n",
      "0.9818181818181817                                                                                       \n",
      "{'learning_rate': 0.162, 'max_depth': 5, 'max_features': 'auto', 'n_estimators': 146, 'subsample': 0.8200000000000001}\n",
      "new best:                                                                                                \n",
      "0.9834090909090909                                                                                       \n",
      "{'learning_rate': 0.23600000000000002, 'max_depth': 3, 'max_features': 'auto', 'n_estimators': 129, 'subsample': 0.74}\n",
      "new best:                                                                                                \n",
      "0.9834090909090911                                                                                       \n",
      "{'learning_rate': 0.383, 'max_depth': 6, 'max_features': 'auto', 'n_estimators': 183, 'subsample': 0.88} \n",
      "100%|███████████████████████████████| 100/100 [13:50<00:00,  8.31s/trial, best loss: -0.9834090909090911]\n",
      "\n",
      "Using the *LDA* for estimation.\n",
      "new best:                                                                                                \n",
      "0.8409090909090909                                                                                       \n",
      "{'shrinkage': 0.02, 'solver': 'eigen'}                                                                   \n",
      "new best:                                                                                                \n",
      "0.8515909090909091                                                                                       \n",
      "{'shrinkage': 0.64, 'solver': 'lsqr'}                                                                    \n",
      "new best:                                                                                                \n",
      "0.8572727272727272                                                                                       \n",
      "{'shrinkage': 0.76, 'solver': 'eigen'}                                                                   \n",
      "new best:                                                                                                \n",
      "0.8581818181818182                                                                                       \n",
      "{'shrinkage': 0.91, 'solver': 'eigen'}                                                                   \n",
      "new best:                                                                                                \n",
      "0.8643181818181818                                                                                       \n",
      "{'shrinkage': 0.99, 'solver': 'eigen'}                                                                   \n",
      "new best:                                                                                                \n",
      "0.8650000000000001                                                                                       \n",
      "{'shrinkage': 0.99, 'solver': 'eigen'}                                                                   \n",
      "new best:                                                                                                \n",
      "0.8654545454545454                                                                                       \n",
      "{'shrinkage': 0.98, 'solver': 'eigen'}                                                                   \n",
      "100%|███████████████████████████████| 100/100 [00:13<00:00,  7.23trial/s, best loss: -0.8654545454545454]\n",
      "\n",
      "Using the *Random Forest* for estimation.\n",
      "new best:                                                                                                \n",
      "0.9715909090909091                                                                                       \n",
      "{'max_features': 'log2', 'min_samples_split': 3, 'n_estimators': 150}                                    \n",
      "new best:                                                                                                \n",
      "0.9738636363636363                                                                                       \n",
      "{'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 233}                                    \n",
      "new best:                                                                                                \n",
      "0.9740909090909091                                                                                       \n",
      "{'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 221}                                    \n",
      "new best:                                                                                                \n",
      "0.9740909090909092                                                                                       \n",
      "{'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 83}                                     \n",
      "new best:                                                                                                \n",
      "0.975                                                                                                    \n",
      "{'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 167}                                    \n",
      "new best:                                                                                                \n",
      "0.9752272727272728                                                                                       \n",
      "{'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 102}                                    \n",
      "100%|███████████████████████████████| 100/100 [09:47<00:00,  5.87s/trial, best loss: -0.9752272727272728]\n",
      "\n",
      "Using the *AdaBoost* for estimation.\n",
      "new best:                                                                                                \n",
      "0.9793181818181818                                                                                       \n",
      "{'learning_rate': 0.386, 'n_estimators': 122}                                                            \n",
      "new best:                                                                                                \n",
      "0.9831818181818183                                                                                       \n",
      "{'learning_rate': 0.392, 'n_estimators': 113}                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best:                                                                                                \n",
      "0.9845454545454545                                                                                       \n",
      "{'learning_rate': 0.275, 'n_estimators': 163}                                                            \n",
      " 94%|██████████████████████████████  | 94/100 [09:58<00:39,  6.60s/trial, best loss: -0.9845454545454545]"
     ]
    }
   ],
   "source": [
    "# Initialize a dataframe for results collection\n",
    "model_results = pd.DataFrame(columns = ['classifier', 'best_cv_score', 'runtime_hpo', 'best_params', 'test_acc'])\n",
    "model_results.head()\n",
    "\n",
    "# Run the HPO and get the best params for each classifier\n",
    "for i in range(len(classifiers)):\n",
    "    \n",
    "    print()\n",
    "    print(f'Using the *{names[i]}* for estimation.')\n",
    "    \n",
    "    def hyperopt_cv_score(params):\n",
    "        cv = RepeatedKFold(n_splits=5, n_repeats=5)\n",
    "        model = classifiers[i].set_params(**params) # use the classifier from the list\n",
    "        return cross_val_score(model, \n",
    "                               X_train, y_train, \n",
    "                               cv = cv,  \n",
    "                               scoring = 'accuracy',\n",
    "                               error_score='raise').mean() #np.mean(gbc.predict(X_test) == y_test)\n",
    "\n",
    "    def f(params):\n",
    "        global best_cv_score\n",
    "        global best_params\n",
    "        global best_time\n",
    "        cv_score = hyperopt_cv_score(params)\n",
    "        if cv_score > best_cv_score:\n",
    "            best_cv_score = cv_score # what is the best score?\n",
    "            best_params = params # what are the best params?\n",
    "            best_time = round(time.time() - start_time,4) # track how much time it took to find the best params\n",
    "            print('new best:', best_cv_score, best_params)\n",
    "        return {'loss': -cv_score, # see the comment below\n",
    "                'status': STATUS_OK}\n",
    "        # Comment regarding 'negative accuracy' (from the referenced source):\n",
    "        ## Since we are trying to maximize the accuracy (acc in the code above), \n",
    "        ## we must negate this value for hyperopt, since hyperopt only knows how to minimize a function. \n",
    "        ## Minimizing a function f is the same as maximizing the negative of f.\n",
    "\n",
    "    best_cv_score = 0\n",
    "    best_params = None\n",
    "    best_time = 0\n",
    "    trials = Trials() # store info at each step\n",
    "    \n",
    "    # Start running the algorithm and track time\n",
    "    start_time = time.time()\n",
    "    best = fmin(f, \n",
    "                search_spaces[i], # use the search space associated with the classifier\n",
    "                algo = tpe.suggest, \n",
    "                max_evals = 100, # how many evaluations?\n",
    "                trials = trials)\n",
    "    \n",
    "    # save trials\n",
    "    \n",
    "    \n",
    "    # Compute the accuracy score on the best model of the classifier\n",
    "    m = classifiers[i].set_params(**best_params).fit(X_train, y_train)\n",
    "    score_test = m.score(X_test, y_test)\n",
    "    \n",
    "    \n",
    "    # Append the best results to the df\n",
    "    model_results = model_results.append({'classifier': names[i], \n",
    "                             'best_cv_score': best_cv_score, \n",
    "                             'runtime_hpo': best_time, \n",
    "                             'best_params': best_params,\n",
    "                             'test_acc': score_test}, ignore_index = True)\n",
    "\n",
    "# Sort the model results by test accuracy score and then Runtime\n",
    "model_results = model_results.sort_values(['test_acc', 'runtime_hpo'], ascending = [0,1]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c44771",
   "metadata": {},
   "source": [
    "Let's look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910a2f4",
   "metadata": {},
   "source": [
    "Let's plot the model test set accuracy against runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc5b7b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = [5, 5])\n",
    "\n",
    "for i in range(len(model_results)):\n",
    "    # Add classifier values\n",
    "    plt.scatter(model_results['runtime_hpo'][i], \n",
    "            model_results['test_acc'][i], \n",
    "            label = model_results['classifier'][i],\n",
    "               marker = 'x',\n",
    "               s = 60)\n",
    "    # Add a dashed lined marking the best accuracy threshold for classifier\n",
    "    plt.hlines(y = model_results['test_acc'][i], \n",
    "               xmin=0, \n",
    "               xmax = np.max(model_results['runtime_hpo'])+1,\n",
    "              linestyles = 'dashed', alpha = 0.3, color = 'grey')\n",
    "    # Add classifier name\n",
    "    plt.annotate(model_results['classifier'][i], \n",
    "                 (model_results['runtime_hpo'][i]+1, \n",
    "                  model_results['test_acc'][i]),\n",
    "                )\n",
    "plt.ylim(bottom = 0.70, top = 1.01)\n",
    "plt.xlabel('Runtime (s)')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.title('Runtime until the best parameters were found')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ad627",
   "metadata": {},
   "source": [
    "Let's plot the accuracy values over each iteration with the baseline accuracy as the reference point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6502ec",
   "metadata": {},
   "source": [
    "Let's plot the values of parameters and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b884ec3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot for parameters\n",
    "## Tutorial: https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce\n",
    "#parameters = ['learning_rate', 'n_estimators',\n",
    "#              'subsample', 'max_depth', \n",
    "#              'max_features', 'criterion']\n",
    "\n",
    "#f, axes = plt.subplots(nrows=2, ncols=3, figsize=(10,5))\n",
    "#f.tight_layout()\n",
    "#cmap = plt.cm.jet\n",
    "#for i, val in enumerate(parameters):\n",
    "#    # print (i, val)\n",
    "#    # Getting the hyperparameter and loss values with list comprehension\n",
    "#    xs = np.array([t['misc']['vals'][val] for t in trials.trials]).ravel()\n",
    "#    ys = [-t['result']['loss'] for t in trials.trials]\n",
    "#    # Making the necessary sorting\n",
    "#    xs, ys =  zip(*sorted(zip(xs, ys)))\n",
    "#    ys = np.array(ys)\n",
    "#    # Plot\n",
    "#    axes[int(i/3),int(i%3)].scatter(xs, ys, \n",
    "#                          s=20, \n",
    "#                          linewidth=0.01, \n",
    "#                         # alpha= 0.5,\n",
    "#                          color=cmap(float(i)/len(parameters))\n",
    "#                                   )\n",
    "#    axes[int(i/3),int(i%3)].set_title(val)\n",
    "#    axes[int(i/3),int(i%3)].set_ylim([0.2,1.0])\n",
    "#    \n",
    "#plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2dd412",
   "metadata": {},
   "source": [
    "## Experimenting with `hyperopt-sklearn`\n",
    "Above, we provided the definition for the search space. Hoqever, `hyperopt` can also function without explicitly specifying the search space - enter `hyperopt-sklearn`, a presently developed library (see [here](https://github.com/hyperopt/hyperopt-sklearn) that provides more automation for hyperparameter optimization. \n",
    "\n",
    "Below we will run the classifier used above without specified search space. Of note, this might mean that some models will not compute or converge, since some hyperparameter combinations do not work together. In those cases, we just skip the hyperparameter combination and proceed.\n",
    "\n",
    "The advantages is that we can reduce manual work in specifying hyperparameter space for each classifier. The main disadvantage is that some of the pereviously-used functions are not easily programmable here, i.e., the `hypropt-sklearn` is somewhat less flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9c79248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/hyperopt/hyperopt-sklearn\n",
    "from hpsklearn import HyperoptEstimator, gradient_boosting_classifier, ada_boost_classifier, random_forest_classifier, logistic_regression, linear_discriminant_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58009fa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the *Gradient Boosting Classifier* for estimation.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.27s/trial, best loss: 0.15384615384615385]\n",
      " 50%|██████████████████▌                  | 1/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.14s/trial, best loss: 0.15384615384615385]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.10s/trial, best loss: 0.15384615384615385]\n",
      " 75%|███████████████████████████▊         | 3/4 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  1.91s/trial, best loss: 0.15384615384615385]\n",
      " 80%|█████████████████████████████▌       | 4/5 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  1.11s/trial, best loss: 0.15384615384615385]\n",
      " 83%|██████████████████████████████▊      | 5/6 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  1.28s/trial, best loss: 0.11538461538461542]\n",
      "100%|██████████| 7/7 [00:01<00:00,  1.14s/trial, best loss: 0.11538461538461542]\n",
      " 88%|████████████████████████████████▍    | 7/8 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  1.13s/trial, best loss: 0.11538461538461542]\n",
      " 89%|████████████████████████████████▉    | 8/9 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  1.10s/trial, best loss: 0.11538461538461542]\n",
      "100%|████████| 10/10 [00:01<00:00,  1.17s/trial, best loss: 0.11538461538461542]\n",
      " 91%|███████████████████████████████▊   | 10/11 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████| 11/11 [00:01<00:00,  1.13s/trial, best loss: 0.11538461538461542]\n",
      " 92%|████████████████████████████████   | 11/12 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████| 12/12 [00:01<00:00,  1.14s/trial, best loss: 0.11538461538461542]\n",
      "100%|████████| 13/13 [00:01<00:00,  1.23s/trial, best loss: 0.11538461538461542]\n",
      " 93%|████████████████████████████████▌  | 13/14 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████| 14/14 [00:01<00:00,  1.19s/trial, best loss: 0.11538461538461542]\n",
      "100%|████████| 15/15 [00:01<00:00,  1.18s/trial, best loss: 0.11538461538461542]\n",
      "100%|████████| 16/16 [00:01<00:00,  1.12s/trial, best loss: 0.11538461538461542]\n",
      " 94%|████████████████████████████████▉  | 16/17 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████| 17/17 [00:01<00:00,  1.10s/trial, best loss: 0.11538461538461542]\n",
      " 94%|█████████████████████████████████  | 17/18 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████| 18/18 [00:01<00:00,  1.09s/trial, best loss: 0.11538461538461542]\n",
      "100%|████████| 19/19 [00:01<00:00,  1.12s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 20/20 [00:01<00:00,  1.27s/trial, best loss: 0.10256410256410253]\n",
      " 95%|█████████████████████████████████▎ | 20/21 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████| 21/21 [00:01<00:00,  1.12s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 22/22 [00:01<00:00,  1.10s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 23/23 [00:01<00:00,  1.17s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 24/24 [00:01<00:00,  1.46s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 25/25 [00:01<00:00,  1.10s/trial, best loss: 0.08974358974358976]\n",
      "100%|█████████| 26/26 [00:01<00:00,  1.10s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 27/27 [00:01<00:00,  1.10s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 28/28 [00:01<00:00,  1.16s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 29/29 [00:01<00:00,  1.11s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 30/30 [00:01<00:00,  1.10s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 31/31 [00:01<00:00,  1.10s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 32/32 [00:01<00:00,  1.10s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 33/33 [00:01<00:00,  1.10s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 34/34 [00:01<00:00,  1.11s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 35/35 [00:01<00:00,  1.33s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 36/36 [00:01<00:00,  1.11s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 37/37 [00:01<00:00,  1.16s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 38/38 [00:01<00:00,  1.12s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 39/39 [00:01<00:00,  1.14s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 40/40 [00:01<00:00,  1.17s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 41/41 [00:01<00:00,  1.37s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 42/42 [00:01<00:00,  1.18s/trial, best loss: 0.0641025641025641]\n",
      " 98%|██████████████████████████████████▏| 42/43 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████| 43/43 [00:01<00:00,  1.42s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 44/44 [00:01<00:00,  1.19s/trial, best loss: 0.0641025641025641]\n",
      " 98%|██████████████████████████████████▏| 44/45 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████| 45/45 [00:01<00:00,  1.42s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 46/46 [00:01<00:00,  1.23s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 47/47 [00:01<00:00,  1.62s/trial, best loss: 0.0641025641025641]\n",
      " 98%|██████████████████████████████████▎| 47/48 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████| 48/48 [00:01<00:00,  1.20s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 49/49 [00:01<00:00,  1.16s/trial, best loss: 0.0641025641025641]\n",
      " 98%|██████████████████████████████████▎| 49/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████| 50/50 [00:01<00:00,  1.28s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 51/51 [00:01<00:00,  1.20s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 52/52 [00:01<00:00,  1.18s/trial, best loss: 0.0641025641025641]\n",
      " 98%|██████████████████████████████████▎| 52/53 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████| 53/53 [00:01<00:00,  1.12s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 54/54 [00:01<00:00,  1.12s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 55/55 [00:01<00:00,  1.13s/trial, best loss: 0.0641025641025641]\n",
      " 98%|██████████████████████████████████▍| 55/56 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████| 56/56 [00:01<00:00,  1.14s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 57/57 [00:01<00:00,  1.54s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 58/58 [00:01<00:00,  1.35s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 59/59 [00:01<00:00,  1.11s/trial, best loss: 0.0641025641025641]\n",
      " 98%|██████████████████████████████████▍| 59/60 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████| 60/60 [00:01<00:00,  1.46s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 61/61 [00:01<00:00,  1.14s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 62/62 [00:01<00:00,  1.12s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 63/63 [00:01<00:00,  1.13s/trial, best loss: 0.0641025641025641]\n",
      " 98%|██████████████████████████████████▍| 63/64 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████| 64/64 [00:01<00:00,  1.11s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 65/65 [00:01<00:00,  1.13s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 66/66 [00:01<00:00,  1.12s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 67/67 [00:01<00:00,  1.15s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 68/68 [00:01<00:00,  1.12s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 69/69 [00:01<00:00,  1.19s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 70/70 [00:01<00:00,  1.13s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 71/71 [00:01<00:00,  1.11s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 72/72 [00:01<00:00,  1.11s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 73/73 [00:01<00:00,  1.19s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 74/74 [00:01<00:00,  1.13s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 75/75 [00:01<00:00,  1.16s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 76/76 [00:01<00:00,  1.12s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 77/77 [00:01<00:00,  1.11s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 78/78 [00:01<00:00,  1.12s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 79/79 [00:01<00:00,  1.11s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 80/80 [00:01<00:00,  1.16s/trial, best loss: 0.0641025641025641]\n",
      " 99%|██████████████████████████████████▌| 80/81 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████| 81/81 [00:01<00:00,  1.28s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 82/82 [00:01<00:00,  1.20s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 83/83 [00:01<00:00,  1.24s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 84/84 [00:01<00:00,  1.24s/trial, best loss: 0.0641025641025641]\n",
      " 99%|██████████████████████████████████▌| 84/85 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████| 85/85 [00:01<00:00,  1.16s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 86/86 [00:01<00:00,  1.12s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 87/87 [00:01<00:00,  1.13s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 88/88 [00:01<00:00,  1.12s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 89/89 [00:01<00:00,  1.12s/trial, best loss: 0.0641025641025641]\n",
      " 99%|██████████████████████████████████▌| 89/90 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████| 90/90 [00:01<00:00,  1.18s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 91/91 [00:01<00:00,  1.16s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 92/92 [00:01<00:00,  1.24s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 93/93 [00:01<00:00,  1.24s/trial, best loss: 0.0641025641025641]\n",
      " 99%|██████████████████████████████████▋| 93/94 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████| 94/94 [00:01<00:00,  1.13s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 95/95 [00:01<00:00,  1.12s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 96/96 [00:01<00:00,  1.12s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 97/97 [00:01<00:00,  1.15s/trial, best loss: 0.0641025641025641]\n",
      "100%|█████████| 98/98 [00:01<00:00,  1.13s/trial, best loss: 0.0641025641025641]\n",
      " 99%|██████████████████████████████████▋| 98/99 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████| 99/99 [00:01<00:00,  1.12s/trial, best loss: 0.0641025641025641]\n",
      " 99%|█████████████████████████████████▋| 99/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████| 100/100 [00:01<00:00,  1.13s/trial, best loss: 0.0641025641025641]\n",
      "\n",
      "Using the *LDA* for estimation.\n",
      "  0%|                                     | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: The leading minor of order 14 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 0/1 [00:01<?, ?trial/s, best loss=?]\n",
      "Model with params not computed\n",
      "\n",
      "Using the *Random Forest* for estimation.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/trial, best loss: 0.14102564102564108]\n",
      "100%|██████████| 2/2 [00:02<00:00,  2.50s/trial, best loss: 0.14102564102564108]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.25s/trial, best loss: 0.14102564102564108]\n",
      "100%|██████████| 4/4 [00:01<00:00,  1.27s/trial, best loss: 0.14102564102564108]\n",
      "100%|██████████| 5/5 [00:01<00:00,  1.10s/trial, best loss: 0.14102564102564108]\n",
      "100%|███████████| 6/6 [00:01<00:00,  1.16s/trial, best loss: 0.1282051282051282]\n",
      "100%|██████████| 7/7 [00:01<00:00,  1.46s/trial, best loss: 0.10256410256410253]\n",
      "100%|██████████| 8/8 [00:01<00:00,  1.22s/trial, best loss: 0.10256410256410253]\n",
      "100%|██████████| 9/9 [00:01<00:00,  1.10s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 10/10 [00:01<00:00,  1.22s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 11/11 [00:01<00:00,  1.17s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 12/12 [00:01<00:00,  1.10s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 13/13 [00:01<00:00,  1.14s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 14/14 [00:01<00:00,  1.10s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 15/15 [00:01<00:00,  1.17s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 16/16 [00:01<00:00,  1.39s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 17/17 [00:01<00:00,  1.49s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 18/18 [00:01<00:00,  1.46s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 19/19 [00:01<00:00,  1.18s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 20/20 [00:01<00:00,  1.14s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 21/21 [00:03<00:00,  3.70s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 22/22 [00:01<00:00,  1.90s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 23/23 [00:01<00:00,  1.15s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 24/24 [00:01<00:00,  1.16s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 25/25 [00:01<00:00,  1.83s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 26/26 [00:02<00:00,  2.61s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 27/27 [00:01<00:00,  1.21s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 28/28 [00:01<00:00,  1.24s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 29/29 [00:01<00:00,  1.38s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 30/30 [00:01<00:00,  1.73s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 31/31 [00:05<00:00,  5.77s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 32/32 [00:06<00:00,  6.02s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 33/33 [00:04<00:00,  4.45s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 34/34 [00:02<00:00,  2.37s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 35/35 [00:01<00:00,  1.13s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 36/36 [00:01<00:00,  1.14s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 37/37 [00:01<00:00,  1.13s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 38/38 [00:02<00:00,  2.17s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 39/39 [00:01<00:00,  1.20s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 40/40 [00:01<00:00,  1.13s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 41/41 [00:01<00:00,  1.14s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 42/42 [00:01<00:00,  1.17s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 43/43 [00:01<00:00,  1.10s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 44/44 [00:01<00:00,  1.13s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 45/45 [00:01<00:00,  1.14s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 46/46 [00:01<00:00,  1.21s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 47/47 [00:01<00:00,  1.26s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 48/48 [00:01<00:00,  1.15s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 49/49 [00:02<00:00,  2.22s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 50/50 [00:01<00:00,  1.13s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 51/51 [00:01<00:00,  1.13s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 52/52 [00:01<00:00,  1.11s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 53/53 [00:01<00:00,  1.14s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 54/54 [00:01<00:00,  1.14s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 55/55 [00:01<00:00,  1.11s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 56/56 [00:01<00:00,  1.12s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 57/57 [00:01<00:00,  1.14s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 58/58 [00:01<00:00,  1.18s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 59/59 [00:01<00:00,  1.18s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 60/60 [00:01<00:00,  1.12s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 61/61 [00:01<00:00,  1.36s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 62/62 [00:01<00:00,  1.12s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 63/63 [00:01<00:00,  1.13s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 64/64 [00:01<00:00,  1.38s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 65/65 [00:01<00:00,  1.16s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 66/66 [00:01<00:00,  1.51s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 67/67 [00:01<00:00,  1.45s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 68/68 [00:03<00:00,  3.01s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 69/69 [00:02<00:00,  2.19s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 70/70 [00:01<00:00,  1.14s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 71/71 [00:01<00:00,  1.31s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 72/72 [00:01<00:00,  1.16s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 73/73 [00:01<00:00,  1.21s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 74/74 [00:01<00:00,  1.14s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 75/75 [00:01<00:00,  1.18s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 76/76 [00:01<00:00,  1.13s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 77/77 [00:01<00:00,  1.12s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 78/78 [00:01<00:00,  1.28s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 79/79 [00:01<00:00,  1.25s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 80/80 [00:01<00:00,  1.23s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 81/81 [00:01<00:00,  1.35s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 82/82 [00:01<00:00,  1.14s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 83/83 [00:01<00:00,  1.22s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 84/84 [00:01<00:00,  1.25s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 85/85 [00:01<00:00,  1.18s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 86/86 [00:01<00:00,  1.17s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 87/87 [00:01<00:00,  1.14s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 88/88 [00:01<00:00,  1.16s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 89/89 [00:01<00:00,  1.20s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 90/90 [00:01<00:00,  1.26s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 91/91 [00:01<00:00,  1.22s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 92/92 [00:01<00:00,  1.22s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 93/93 [00:01<00:00,  1.12s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 94/94 [00:01<00:00,  1.42s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 95/95 [00:01<00:00,  1.13s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 96/96 [00:01<00:00,  1.35s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 97/97 [00:01<00:00,  1.13s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 98/98 [00:01<00:00,  1.16s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 99/99 [00:01<00:00,  1.14s/trial, best loss: 0.08974358974358976]\n",
      "100%|██████| 100/100 [00:01<00:00,  1.15s/trial, best loss: 0.08974358974358976]\n",
      "\n",
      "Using the *AdaBoost* for estimation.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.09s/trial, best loss: 0.10256410256410253]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.64s/trial, best loss: 0.10256410256410253]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.73s/trial, best loss: 0.10256410256410253]\n",
      "100%|██████████| 4/4 [00:01<00:00,  1.79s/trial, best loss: 0.10256410256410253]\n",
      "100%|██████████| 5/5 [00:01<00:00,  1.35s/trial, best loss: 0.10256410256410253]\n",
      "100%|██████████| 6/6 [00:01<00:00,  1.95s/trial, best loss: 0.10256410256410253]\n",
      "100%|██████████| 7/7 [00:01<00:00,  1.50s/trial, best loss: 0.10256410256410253]\n",
      "100%|██████████| 8/8 [00:01<00:00,  1.25s/trial, best loss: 0.10256410256410253]\n",
      "100%|██████████| 9/9 [00:01<00:00,  1.09s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 10/10 [00:01<00:00,  1.10s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 11/11 [00:01<00:00,  1.24s/trial, best loss: 0.10256410256410253]\n",
      "100%|████████| 12/12 [00:01<00:00,  1.18s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 13/13 [00:01<00:00,  1.39s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 14/14 [00:01<00:00,  1.10s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 15/15 [00:01<00:00,  1.24s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 16/16 [00:01<00:00,  1.11s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 17/17 [00:01<00:00,  1.10s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 18/18 [00:01<00:00,  1.10s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 19/19 [00:01<00:00,  1.10s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 20/20 [00:01<00:00,  1.74s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 21/21 [00:01<00:00,  1.16s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 22/22 [00:01<00:00,  1.16s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 23/23 [00:01<00:00,  1.14s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 24/24 [00:01<00:00,  1.19s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 25/25 [00:01<00:00,  1.20s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 26/26 [00:01<00:00,  1.14s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 27/27 [00:01<00:00,  1.20s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 28/28 [00:01<00:00,  1.26s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 29/29 [00:01<00:00,  1.11s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 30/30 [00:01<00:00,  1.18s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 31/31 [00:01<00:00,  1.39s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 32/32 [00:01<00:00,  1.09s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 33/33 [00:01<00:00,  1.28s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 34/34 [00:01<00:00,  1.19s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 35/35 [00:01<00:00,  1.14s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 36/36 [00:01<00:00,  1.19s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 37/37 [00:01<00:00,  1.13s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 38/38 [00:01<00:00,  1.39s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 39/39 [00:01<00:00,  1.75s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 40/40 [00:01<00:00,  1.11s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 41/41 [00:01<00:00,  1.13s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 42/42 [00:01<00:00,  1.39s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 43/43 [00:01<00:00,  1.38s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 44/44 [00:01<00:00,  1.10s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 45/45 [00:01<00:00,  1.19s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 46/46 [00:01<00:00,  1.13s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 47/47 [00:01<00:00,  1.18s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 48/48 [00:01<00:00,  1.44s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 49/49 [00:01<00:00,  1.30s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 50/50 [00:01<00:00,  1.15s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 51/51 [00:01<00:00,  1.34s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 52/52 [00:01<00:00,  1.18s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 53/53 [00:01<00:00,  1.26s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 54/54 [00:01<00:00,  1.18s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 55/55 [00:01<00:00,  1.49s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 56/56 [00:01<00:00,  1.11s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 57/57 [00:01<00:00,  1.36s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 58/58 [00:01<00:00,  1.12s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 59/59 [00:01<00:00,  1.12s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 60/60 [00:01<00:00,  1.82s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 61/61 [00:01<00:00,  1.79s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 62/62 [00:01<00:00,  1.92s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 63/63 [00:01<00:00,  1.43s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 64/64 [00:01<00:00,  1.29s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 65/65 [00:01<00:00,  1.14s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 66/66 [00:01<00:00,  1.15s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 67/67 [00:01<00:00,  1.13s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 68/68 [00:01<00:00,  1.23s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 69/69 [00:01<00:00,  1.21s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 70/70 [00:01<00:00,  1.60s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 71/71 [00:01<00:00,  1.26s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 72/72 [00:01<00:00,  1.14s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 73/73 [00:01<00:00,  1.27s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 74/74 [00:01<00:00,  1.19s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 75/75 [00:01<00:00,  1.24s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 76/76 [00:01<00:00,  1.42s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 77/77 [00:01<00:00,  1.12s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 78/78 [00:01<00:00,  1.12s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 79/79 [00:01<00:00,  1.15s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 80/80 [00:01<00:00,  1.17s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 81/81 [00:01<00:00,  1.13s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 82/82 [00:01<00:00,  1.13s/trial, best loss: 0.08974358974358976]\n",
      "100%|████████| 83/83 [00:01<00:00,  1.14s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 84/84 [00:01<00:00,  1.12s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 85/85 [00:01<00:00,  1.17s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 86/86 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      " 99%|██████████████████████████████████▌| 86/87 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py:675: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight = np.exp(\n",
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py:506: UserWarning: Sample weights have reached infinite values, at iteration 1, causing overflow. Iterations stopped. Try lowering the learning rate.\n",
      "  return super().fit(X, y, sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████| 87/87 [00:01<00:00,  1.21s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 88/88 [00:01<00:00,  1.17s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 89/89 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      " 99%|██████████████████████████████████▌| 89/90 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py:676: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(sample_weight)\n",
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py:675: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight = np.exp(\n",
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py:506: UserWarning: Sample weights have reached infinite values, at iteration 8, causing overflow. Iterations stopped. Try lowering the learning rate.\n",
      "  return super().fit(X, y, sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████| 90/90 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      " 99%|██████████████████████████████████▌| 90/91 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py:675: RuntimeWarning: overflow encountered in exp\n",
      "  sample_weight = np.exp(\n",
      "/Users/drdr/opt/anaconda3/envs/automl_2022/lib/python3.8/site-packages/sklearn/ensemble/_weight_boosting.py:506: UserWarning: Sample weights have reached infinite values, at iteration 2, causing overflow. Iterations stopped. Try lowering the learning rate.\n",
      "  return super().fit(X, y, sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████| 91/91 [00:01<00:00,  1.20s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 92/92 [00:01<00:00,  1.12s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 93/93 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 94/94 [00:01<00:00,  1.14s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 95/95 [00:01<00:00,  1.15s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 96/96 [00:01<00:00,  1.12s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 97/97 [00:01<00:00,  1.13s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 98/98 [00:01<00:00,  1.12s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 99/99 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|██████| 100/100 [00:01<00:00,  1.12s/trial, best loss: 0.07692307692307687]\n",
      "\n",
      "Using the *Logistic Regression* for estimation.\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.09s/trial, best loss: 0.15384615384615385]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.08s/trial, best loss: 0.15384615384615385]\n",
      "100%|███████████| 3/3 [00:01<00:00,  1.09s/trial, best loss: 0.1282051282051282]\n",
      "100%|███████████| 4/4 [00:01<00:00,  1.09s/trial, best loss: 0.1282051282051282]\n",
      "100%|███████████| 5/5 [00:01<00:00,  1.08s/trial, best loss: 0.1282051282051282]\n",
      "100%|███████████| 6/6 [00:01<00:00,  1.08s/trial, best loss: 0.1282051282051282]\n",
      "100%|███████████| 7/7 [00:01<00:00,  1.09s/trial, best loss: 0.1282051282051282]\n",
      "100%|██████████| 8/8 [00:01<00:00,  1.09s/trial, best loss: 0.11538461538461542]\n",
      "100%|██████████| 9/9 [00:01<00:00,  1.08s/trial, best loss: 0.11538461538461542]\n",
      "100%|████████| 10/10 [00:01<00:00,  1.07s/trial, best loss: 0.11538461538461542]\n",
      "100%|████████| 11/11 [00:01<00:00,  1.07s/trial, best loss: 0.11538461538461542]\n",
      "100%|████████| 12/12 [00:01<00:00,  1.08s/trial, best loss: 0.11538461538461542]\n",
      "100%|████████| 13/13 [00:01<00:00,  1.10s/trial, best loss: 0.11538461538461542]\n",
      "100%|████████| 14/14 [00:01<00:00,  1.08s/trial, best loss: 0.11538461538461542]\n",
      "100%|████████| 15/15 [00:01<00:00,  1.08s/trial, best loss: 0.11538461538461542]\n",
      "100%|████████| 16/16 [00:01<00:00,  1.08s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 17/17 [00:01<00:00,  1.08s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 18/18 [00:01<00:00,  1.12s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 19/19 [00:01<00:00,  1.09s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 20/20 [00:01<00:00,  1.16s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 21/21 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 22/22 [00:01<00:00,  1.09s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 23/23 [00:01<00:00,  1.09s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 24/24 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 25/25 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 26/26 [00:01<00:00,  1.09s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 27/27 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 28/28 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 29/29 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 30/30 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 31/31 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 32/32 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 33/33 [00:01<00:00,  1.09s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 34/34 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 35/35 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 36/36 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 37/37 [00:01<00:00,  1.08s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 38/38 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 39/39 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 40/40 [00:01<00:00,  1.09s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 41/41 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 42/42 [00:01<00:00,  1.09s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 43/43 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 44/44 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 45/45 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 46/46 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 47/47 [00:01<00:00,  1.15s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 48/48 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 49/49 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 50/50 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 51/51 [00:01<00:00,  1.09s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 52/52 [00:01<00:00,  1.09s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 53/53 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 54/54 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 55/55 [00:01<00:00,  1.12s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 56/56 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 57/57 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 58/58 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 59/59 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 60/60 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 61/61 [00:01<00:00,  1.08s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 62/62 [00:01<00:00,  1.14s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 63/63 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 64/64 [00:01<00:00,  1.08s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 65/65 [00:01<00:00,  1.09s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 66/66 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 67/67 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 68/68 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 69/69 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 70/70 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 71/71 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 72/72 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 73/73 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 74/74 [00:01<00:00,  1.09s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 75/75 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 76/76 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 77/77 [00:01<00:00,  1.16s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 78/78 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 79/79 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 80/80 [00:01<00:00,  1.15s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 81/81 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 82/82 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 83/83 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 84/84 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 85/85 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 86/86 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 87/87 [00:01<00:00,  1.15s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 88/88 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 89/89 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 90/90 [00:01<00:00,  1.09s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 91/91 [00:01<00:00,  1.09s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 92/92 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 93/93 [00:01<00:00,  1.08s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 94/94 [00:01<00:00,  1.09s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 95/95 [00:01<00:00,  1.16s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 96/96 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 97/97 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 98/98 [00:01<00:00,  1.11s/trial, best loss: 0.07692307692307687]\n",
      "100%|████████| 99/99 [00:01<00:00,  1.09s/trial, best loss: 0.07692307692307687]\n",
      "100%|██████| 100/100 [00:01<00:00,  1.10s/trial, best loss: 0.07692307692307687]\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize a dataframe\n",
    "model_results_auto = pd.DataFrame(columns = ['classifier', 'test_acc', 'best_params'])\n",
    "\n",
    "# 2. Classifier labels\n",
    "names_auto = [\n",
    "    \"Gradient Boosting Classifier\",\n",
    "    \"LDA\",\n",
    "    \"Random Forest\",\n",
    "    \"AdaBoost\",\n",
    "    \"Logistic Regression\"\n",
    "]\n",
    "\n",
    "# 3. Classifier classes\n",
    "classifiers_auto = [\n",
    "    gradient_boosting_classifier(\"gbc\"),\n",
    "    linear_discriminant_analysis('lda'),\n",
    "    random_forest_classifier('rf'), \n",
    "    ada_boost_classifier('ada'), \n",
    "    logistic_regression('logreg')\n",
    "]\n",
    "\n",
    "# 4. Iterate over all classifiers\n",
    "for i in range(len(classifiers_auto)):\n",
    "    print()\n",
    "    print(f'Using the *{names_auto[i]}* for estimation.')\n",
    "    \n",
    "    # Objective function\n",
    "    estim = HyperoptEstimator(classifier = classifiers_auto[i],\n",
    "                                    #  preprocessing=[],\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=100)\n",
    "    try:\n",
    "        # Fit the model\n",
    "        estim.fit(X_train, y_train)\n",
    "        \n",
    "        # Add the results to df\n",
    "        model_results_auto = model_results_auto.append(\n",
    "            {'classifier': names_auto[i], \n",
    "            # 'runtime_hpo': best_time,\n",
    "             'test_acc': estim.score(X_test, y_test),\n",
    "             'best_params': estim.best_model()\n",
    "             }, \n",
    "            ignore_index = True)\n",
    "    except:\n",
    "        print('Model with params not computed')\n",
    "        # Append the name but assign None-values to fields\n",
    "        model_results_auto = model_results_auto.append(\n",
    "            {'classifier': names_auto[i], \n",
    "            # 'runtime_hpo': best_time,\n",
    "             'test_acc': None,\n",
    "             'best_params': None\n",
    "             }, \n",
    "            ignore_index = True)\n",
    "\n",
    "model_results_auto = model_results_auto.sort_values(['test_acc'], ascending = False).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34a7ec",
   "metadata": {},
   "source": [
    "Look at the results table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "765af402",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>{'learner': ([DecisionTreeRegressor(criterion=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>{'learner': LogisticRegression(C=1.62874169142...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.891026</td>\n",
       "      <td>{'learner': (DecisionTreeClassifier(max_depth=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.878205</td>\n",
       "      <td>{'learner': (DecisionTreeClassifier(criterion=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LDA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     classifier  test_acc  \\\n",
       "0  Gradient Boosting Classifier  0.897436   \n",
       "1           Logistic Regression  0.897436   \n",
       "2                      AdaBoost  0.891026   \n",
       "3                 Random Forest  0.878205   \n",
       "4                           LDA       NaN   \n",
       "\n",
       "                                         best_params  \n",
       "0  {'learner': ([DecisionTreeRegressor(criterion=...  \n",
       "1  {'learner': LogisticRegression(C=1.62874169142...  \n",
       "2  {'learner': (DecisionTreeClassifier(max_depth=...  \n",
       "3  {'learner': (DecisionTreeClassifier(criterion=...  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results_auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75703c96",
   "metadata": {},
   "source": [
    "# Significance testing \n",
    "In the below section, we will test whether the differences between the models are statistically significant. For that, we will use pairwise McNemar test to compare each model with each other. P-values have the continuity correction.\n",
    "\n",
    "We first start by defining the helper function for the pairwise McNemar test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8327e546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# McNemar test for practice sessions/homeworks into a function\n",
    "def mcnemar_test(ground_truth, preds1, preds2):\n",
    "    \"\"\"Get the McNemar test results for comparison between two models\n",
    "    Args: \n",
    "        ground_truth (np.array): the ground truth, an np.array (usually y_test)\n",
    "        preds1 (np.array): predictions from Model 1\n",
    "        preds2 (np.array): predictions from Model 2\n",
    "    Returns:\n",
    "        statistic (float): McNemar statistic\n",
    "        pvalue (float): p-value of the significance test\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Create the prediction data set\n",
    "    mc_data = pd.DataFrame()\n",
    "    mc_data[\"ground_truth\"] = y_test.tolist()\n",
    "    mc_data[\"y_pred1\"] = preds1.tolist()\n",
    "    mc_data[\"y_pred2\"] = preds2.tolist()\n",
    "    # mc_data.head(5)\n",
    "\n",
    "    ## Create the contingency table\n",
    "    mc_data.loc[mc_data['ground_truth']==mc_data['y_pred1'],'correct_pred1']=\"Yes\"\n",
    "    mc_data.loc[mc_data['ground_truth']!=mc_data['y_pred1'],'correct_pred1']=\"No\"\n",
    "    mc_data.loc[mc_data['ground_truth']==mc_data['y_pred2'],'correct_pred2']=\"Yes\"\n",
    "    mc_data.loc[mc_data['ground_truth']!=mc_data['y_pred2'],'correct_pred2']=\"No\"\n",
    "\n",
    "    contingency_table_df=pd.DataFrame(data={\"nr_correct_pred1\":[\"Yes/Yes\",\"No/Yes\"], \n",
    "                                            \"nr_incorrect_pred1\":[\"Yes/No\",\"No/No\"]}, \n",
    "                                      index=[\"nr_correct_pred2\",\"nr_incorrect_pred2\"])\n",
    "\n",
    "    ## Add the numbers to contingency tables\n",
    "    nr_corr_pred2_corr_pred1=0\n",
    "    nr_corr_pred2_incorr_pred1=0\n",
    "    nr_incorr_pred2_corr_pred1=0\n",
    "    nr_incorr_pred2_incorr_pred1=0\n",
    "    for index, row in mc_data.iterrows():\n",
    "        if  row['correct_pred2']== \"Yes\" and  row['correct_pred1']==\"Yes\":\n",
    "            nr_corr_pred2_corr_pred1+=1\n",
    "        elif row['correct_pred2']== \"Yes\" and  row['correct_pred1']==\"No\":\n",
    "            nr_corr_pred2_incorr_pred1+=1\n",
    "        elif row['correct_pred2']== \"No\" and  row['correct_pred1']==\"Yes\":\n",
    "            nr_incorr_pred2_corr_pred1+=1\n",
    "        elif row['correct_pred2']== \"No\" and  row['correct_pred1']==\"No\":\n",
    "            nr_incorr_pred2_incorr_pred1+=1\n",
    "\n",
    "    contingency_table_df.iloc[0,0]=nr_corr_pred2_corr_pred1\n",
    "    contingency_table_df.iloc[0,1]=nr_corr_pred2_incorr_pred1\n",
    "    contingency_table_df.iloc[1,0]=nr_incorr_pred2_corr_pred1\n",
    "    contingency_table_df.iloc[1,1]=nr_incorr_pred2_incorr_pred1\n",
    "\n",
    "    # Calculate The McNemar test statistic and p-value\n",
    "    mc_results = mcnemar(contingency_table_df, exact=False, correction=True)\n",
    "    return mc_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0e9c7",
   "metadata": {},
   "source": [
    "Next, we iterate over all models and compare their performance against all other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f027fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the classifier\n",
    "df_sig = pd.DataFrame(columns = ['model1', 'model2', 'mc_stat', 'pvalue_adj'])\n",
    "\n",
    "## Ground truth\n",
    "ground_truth = y_test\n",
    "\n",
    "comparisons = []\n",
    "\n",
    "for i in range(len(model_results)):\n",
    "    for j in range(len(model_results)-1):\n",
    "        # Names of models\n",
    "        name1 = model_results['classifier'][i]\n",
    "        name2 = model_results['classifier'][j+1]\n",
    "        \n",
    "        if name1 == name2: # no point in comparing the model with self\n",
    "            pass\n",
    "        else:\n",
    "            comparison = list(set((name1, name2))) # no point in duplicating comparisons\n",
    "        \n",
    "            if comparison not in comparisons:\n",
    "                comparisons.append(comparison)\n",
    "                \n",
    "                # Model 1\n",
    "                idx1 = int(np.where(np.array(names) == name1)[0])\n",
    "                classifier1 = classifiers[idx1]\n",
    "                best_params1 = model_results['best_params'][i]\n",
    "                best_model1 = classifier1.set_params(**best_params1).fit(X_train, y_train)\n",
    "                preds1 = best_model1.predict(X_test)\n",
    "\n",
    "                ## Model 2\n",
    "                idx2 = int(np.where(np.array(names) == model_results['classifier'][j+1])[0])\n",
    "                classifier2 = classifiers[idx2]\n",
    "                best_params2 = model_results['best_params'][j+1]\n",
    "                best_model2 = classifier2.set_params(**best_params2).fit(X_train, y_train)\n",
    "                preds2 = best_model2.predict(X_test)\n",
    "\n",
    "                # McNemar test\n",
    "                mc = mcnemar_test(ground_truth, preds1, preds2)\n",
    "                pvalue, statistic = mc.pvalue, mc.statistic\n",
    "                \n",
    "                # Append to the dataframe\n",
    "                df_sig = df_sig.append({'model1': name1, \n",
    "                               'model2': name2, \n",
    "                               'mc_stat': statistic, \n",
    "                               'pvalue_adj': pvalue}, ignore_index = True)\n",
    "            else:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3304d6dc",
   "metadata": {},
   "source": [
    "Let's look at the pairwise differences table. Just a reminder that p-values have the continuity correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "26ee411f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>mc_stat</th>\n",
       "      <th>pvalue_adj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>140.007042</td>\n",
       "      <td>2.652612e-32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>137.007194</td>\n",
       "      <td>1.201490e-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>LDA</td>\n",
       "      <td>135.007299</td>\n",
       "      <td>3.289573e-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>131.007519</td>\n",
       "      <td>2.466702e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>134.007353</td>\n",
       "      <td>5.443351e-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>LDA</td>\n",
       "      <td>135.007299</td>\n",
       "      <td>3.289573e-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>130.007576</td>\n",
       "      <td>4.082167e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>LDA</td>\n",
       "      <td>129.007634</td>\n",
       "      <td>6.755805e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>129.007634</td>\n",
       "      <td>6.755805e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LDA</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>130.007576</td>\n",
       "      <td>4.082167e-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model1                        model2     mc_stat  \\\n",
       "0                      AdaBoost                 Random Forest  140.007042   \n",
       "1                      AdaBoost  Gradient Boosting Classifier  137.007194   \n",
       "2                      AdaBoost                           LDA  135.007299   \n",
       "3                      AdaBoost           Logistic Regression  131.007519   \n",
       "4                 Random Forest  Gradient Boosting Classifier  134.007353   \n",
       "5                 Random Forest                           LDA  135.007299   \n",
       "6                 Random Forest           Logistic Regression  130.007576   \n",
       "7  Gradient Boosting Classifier                           LDA  129.007634   \n",
       "8  Gradient Boosting Classifier           Logistic Regression  129.007634   \n",
       "9                           LDA           Logistic Regression  130.007576   \n",
       "\n",
       "     pvalue_adj  \n",
       "0  2.652612e-32  \n",
       "1  1.201490e-31  \n",
       "2  3.289573e-31  \n",
       "3  2.466702e-30  \n",
       "4  5.443351e-31  \n",
       "5  3.289573e-31  \n",
       "6  4.082167e-30  \n",
       "7  6.755805e-30  \n",
       "8  6.755805e-30  \n",
       "9  4.082167e-30  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d84d92",
   "metadata": {},
   "source": [
    "<font color= 'red'> Something's probably not right here. RF and GBC have the same test accuracy yet they are different here... </font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
