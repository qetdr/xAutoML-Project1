{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56262c0",
   "metadata": {},
   "source": [
    "# Drinking Water Quality Prediction\n",
    "Project 1 for the *Explainable Automated Machine Learning* (LTAT.02.023) course\n",
    "\n",
    "**Authors**: <br>\n",
    "Dmitri Rozgonjuk <br>\n",
    "Lisanna Lehes <br>\n",
    "Marilin Moor <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a38a89",
   "metadata": {},
   "source": [
    "## Background\n",
    "tbw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d518c6e7",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065c3354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "#!pip install hyperopt\n",
    "#!pip install git+https://github.com/hyperopt/hyperopt.git\n",
    "#!pip install git+https://github.com/hyperopt/hyperopt-sklearn\n",
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69141ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "## Data frames and arrays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Model training and evaluation\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "## Classifiers\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "## Hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import hyperopt.pyll.stochastic\n",
    "from hpsklearn import HyperoptEstimator, gradient_boosting_classifier, ada_boost_classifier,\\\n",
    "random_forest_classifier, logistic_regression, linear_discriminant_analysis # add more classifiers\n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Differences testing\n",
    "from scipy.stats import friedmanchisquare # Friedman chi-squared test \n",
    "import scikit_posthocs as sp # post-hocs (here, e.g, Nemenyi test)\n",
    "\n",
    "## Misc\n",
    "import time\n",
    "from statsmodels.stats.contingency_tables import mcnemar # McNemar test\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f5e65",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "üldine jutt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import, mingid kirjeldavad plotid, missing data, jne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada3cbcd",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Siia tulevad Marilini andmete puhastamise koodi põhjendused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd60153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kood andmete puhastamiseks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3933600a",
   "metadata": {},
   "source": [
    "All on juba puhas kood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14fd2c2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dims: (305, 27)\n",
      "y_train dims: (305,)\n",
      "X_test dims: (156, 27)\n",
      "y_test dims: (156,)\n"
     ]
    }
   ],
   "source": [
    "# MM\n",
    "train_df = pd.read_csv(\"clean_data/train.csv\")\n",
    "test_df = pd.read_csv(\"clean_data/test.csv\")\n",
    "\n",
    "y_train = train_df['compliance_2020']\n",
    "X_train = train_df.drop(columns = ['compliance_2020', 'compliance_2021'])\n",
    "\n",
    "# Test data\n",
    "y_test = test_df['compliance_2020']\n",
    "X_test = test_df.drop(columns = ['compliance_2020'])\n",
    "\n",
    "print(f'X_train dims: {X_train.shape}')\n",
    "print(f'y_train dims: {y_train.shape}')\n",
    "\n",
    "#print(f'X_val dims: {X_val.shape}')\n",
    "#print(f'y_val dims: {y_val.shape}')\n",
    "\n",
    "print(f'X_test dims: {X_test.shape}')\n",
    "print(f'y_test dims: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57db21d4",
   "metadata": {},
   "source": [
    "## Baseline Model Selection\n",
    "In the following section we are selecting and testing the baseline accuracy of 13 classifiers. Insipiration for the classifier selection can be found from [here](https://www.educative.io/blog/scikit-learn-cheat-sheet-classification-regression-methods) and [here](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5741f5",
   "metadata": {},
   "source": [
    "### An overview of classifiers\n",
    "Here we are testing 13 different classification algorithms. Algorithms were chosen so that they would  be different and work on a classification task. Altogether we have chosen algorithms from the following sklearn classes:\n",
    "\n",
    "**Ensemble** algorithms that combine several base models in order to produce the most optimal model. We have 3 boosting and 1 bagging algorithm. **Boosting Models** speciality is that they are trying to improve the prediction power by training a sequence of weak models while each new model is trying to correct the errors made by the previous ones ([source](https://towardsdatascience.com/boosting-algorithms-explained-d38f56ef3f30)). **Bagging models** speciality is that they are training a bunch of individual models in a parallel way, so that each model is trained by a random subset of the data ([source](https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725)).\n",
    "- [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) assigns weights to miss-classified datapoints, so that the next classifier would pay extra attention to these poins and therefore, get them right.\n",
    "- [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html) uses decision trees as its weak predictors\n",
    "- [Gradient Boosting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?highlight=adaboostclassifer) focuses on the difference between the prediction and the ground truth. \n",
    "- [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) uses bagging as the ensemble method and decision tree as the individual model.\n",
    "\n",
    "**Discriminant Analysis** is used to assign objects to one group among a number of known groups. \n",
    "- [LDA](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) is a linear model for classification and works by calculating summary statistics for the input features by class label, such as the mean and standard deviation. These statistics represent the model learned from the training data.\n",
    "- [QDA](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html)  is particularly useful if there is prior knowledge that individual classes exhibit distinct covariances.\n",
    "\n",
    "**Linear models** are a set of methods intended for regression in which the target value is expected to be a linear combination of the features.\n",
    "- [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) is a linear model for classification. \n",
    "- [SGD](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) is a linear classifier optimized by SGD which implements various regularised linear models by modifying the parameter “loss”.\n",
    "\n",
    "**Naive Bayes**. These methods are a set of supervised learning algorithms based on applying Bayes’ theorem that has an assumption of independence among predictors.\n",
    "- [Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) is a probabilistic classification algorithm based on applying Bayes' theorem with strong independence assumptions.\n",
    "\n",
    "**Gaussian Process** allows to shape your prior belief via the choice of kernel. Also, the greatest practical advantage is that they can give a reliable estimate of their own uncertainty.\n",
    "- [Gaussian Process](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html) is a non-parametric algorithm that can be applied to binary classification tasks. \n",
    "\n",
    "**Neighbours** module implements the k-nearest neighbors algorithm.\n",
    "- [Nearest Neighbours](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) assumes that similar things exist in close proximity, meanind that similar things are near to each other.\n",
    "\n",
    "**Tree** module includes decision tree-based models for classification and regression.\n",
    "- [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) can be used to visually and explicitly represent decisions and decision making. \n",
    "\n",
    "**SVM** is a linear model for classification (and regression) problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is based on a line or a hyperplane which separates the data into classes.\n",
    "- [RBF SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC) is an SVM that uses RBF kernel that is one a widely used kernel due to its similarity to the Gaussian distribution. The RBF kernel function for two points computes the similarity or how close they are to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b3dd82",
   "metadata": {},
   "source": [
    "### Classifiers and their search spaces (for further use in the HPO section)\n",
    "Below we are assemblying a dataframe of classifiers (their names and operatinalizations) as well as manually defined search spaces to be used later in the hyperparameter optimization (HPO) task. We decided to add the HPO search spaces already here to provide a consistent code that allows for smoother readability and automation of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530a6028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels of classifiers\n",
    "names = [\n",
    "    \"K-Nearest Neighbors\",\n",
    "    \"SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "    \"LDA\",\n",
    "    \"Logistic Regression\",\n",
    "    \"XGBoost\",\n",
    "    \"SGD\",\n",
    "    \"Gradient Boosting Classifier\"\n",
    "]\n",
    "\n",
    "# Default/baseline classifier model classes\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    SVC(), # kernel = \"rbf\" by default\n",
    "    GaussianProcessClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    LogisticRegression(),\n",
    "    XGBClassifier(),\n",
    "    SGDClassifier(),\n",
    "    GradientBoostingClassifier()\n",
    "]\n",
    "\n",
    "# Clone the default baseline models for later use\n",
    "classifiers_clone = [clone(cl_from_list, safe = True) for cl_from_list in classifiers]\n",
    "\n",
    "# Define individual search spaces manually\n",
    "search_spaces = [\n",
    "    \n",
    "    # KNN\n",
    "    {\n",
    "        'n_neighbors' : hp.choice('n_neighbors', range(2, 20, 1)), # default = 5\n",
    "        'weights': hp.choice('weights', ['uniform', 'distance']), # default = 'uniform'\n",
    "        'algorithm': hp.choice('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute']), # default = 'auto' \n",
    "        'leaf_size': hp.choice('leaf_size', range(10, 40, 1)), # default=30\n",
    "        'p': hp.choice('p', [1, 2]) # default=2\n",
    "    },\n",
    "    \n",
    "    #SVM\n",
    "    {\n",
    "        'C': hp.uniform('C', 0, 1), # default = 1.0\n",
    "        'kernel': hp.choice('kernel', ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']), # default=’rbf’\n",
    "     #   'shrinking': hp.choice('shrinking', [True, False]), # default=True\n",
    "    },\n",
    "    \n",
    "    # Gaussian Process\n",
    "    {\n",
    "        'max_iter_predict': hp.choice('max_iter_predict', range(50, 200, 1)) # default=100\n",
    "    },\n",
    "    \n",
    "    # Decision Tree\n",
    "    {\n",
    "        'splitter': hp.choice('splitter', ['best', 'random']), # default='best'\n",
    "        'max_depth': hp.choice('max_depth',  range(1,20)), # default=None\n",
    "        'min_samples_split': hp.choice('min_samples_split', range(2, 7, 1)), # default=2\n",
    "        'min_samples_leaf': hp.choice('min_samples_leaf', range(2, 7, 1)), # default=2\n",
    "        'max_features': hp.choice('max_features', ['sqrt', 'log2']), # default='sqrt'\n",
    "        \n",
    "    },\n",
    "       \n",
    "    # Random Forest\n",
    "    {\n",
    "        'n_estimators': hp.choice('n_estimators', range(50, 250, 1)), # default=100\n",
    "        'max_depth': hp.choice('max_depth',  range(1,20)), # default=None\n",
    "        'min_samples_split': hp.choice('min_samples_split', range(2, 7, 1)), # default=2\n",
    "        'min_samples_leaf': hp.choice('min_samples_leaf', range(2, 7, 1)), # default=2\n",
    "        'max_features': hp.choice('max_features', ['sqrt', 'log2']), # default='sqrt'\n",
    "    },\n",
    "    \n",
    "    # AdaBoost\n",
    "    {\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.001, 0.4), # default=1.0\n",
    "        'n_estimators': hp.choice('n_estimators', range(50, 250, 1)), # default=100\n",
    "        \n",
    "    },\n",
    "    \n",
    "    # Naive Bayes\n",
    "    {\n",
    "        'var_smoothing': hp.uniform('var_smoothing',1e-8, 1e-10), # default=1e-9\n",
    "    },\n",
    "    \n",
    "    # QDA\n",
    "    {\n",
    "        'reg_param': hp.uniform('reg_param', 0.0, 1.0), # default=0.0\n",
    "        'tol': hp.uniform('reg_param', 0.0001, 0.010), # default=0.0001\n",
    "    }, \n",
    "    \n",
    "    # LDA\n",
    "    {\n",
    "        'solver': hp.choice('solver', ['lsqr', 'eigen']), # default=’svd’\n",
    "        'shrinkage': hp.uniform('shrinkage', 0.01, 1), # default=None # not supported with 'svd' solver\n",
    "        \n",
    "    },\n",
    "    \n",
    "    # Logistic Regression\n",
    "    { \n",
    "        'penalty': hp.choice('penalty', ['l1', 'l2', 'elasticnet', 'none']), # default=’l2’\n",
    "        'l1_ratio': hp.uniform('l1_ratio', 0, 1),\n",
    "        'C': hp.uniform('C', 0, 1), # default = 1.0\n",
    "        'solver': hp.choice('solver', ['saga','saga'])  # # use solver = 'saga' for using different penalties (here, give a pseudo-choice)\n",
    "    },\n",
    "    \n",
    "    # XGBoost (Note: there a lot more hyperparameters here)\n",
    "    {\n",
    "        'booster': hp.choice('booster', ['gbtree', 'gblinear', 'dart']), # default = 'gbtree'\n",
    "        'colsample_bylevel': hp.uniform('colsample_bylevel', 0,.0 1.0), # default = 1.0\n",
    "        'colsample_bytree':  hp.uniform('colsample_bytree', 0.0, 1.0), # default = 1.0\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.0, 1.0), # default=0.1\n",
    "        'skip_drop': hp.uniform('skip_drop', 0.0, 1.0), # default=0.0\n",
    "    },\n",
    "    \n",
    "    # SGD\n",
    "    {\n",
    "        'loss': hp.choice('loss', ['hinge', 'log_loss', 'log', \n",
    "                                   'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', \n",
    "                                   'huber', 'epsilon_insensitive',\n",
    "                                   'squared_epsilon_insensitive']) # default=’hinge’\n",
    "        'penalty': hp.choice('penalty', ['l1', 'l2', 'elasticnet']), # default=’l2’\n",
    "        'alpha': hp.uniform('alpha', 0, 0.01), # default=0.0001\n",
    "        'learning_rate': hp.choice('learning_rate', ['constant', 'optimal', 'invscaling', 'adaptive']) # default=’optimal’\n",
    "        \n",
    "    },\n",
    "\n",
    "    # GBC\n",
    "    {\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.001, 0.4), # default=0.1\n",
    "        'n_estimators': hp.choice('n_estimators', range(50, 250, 1)), # default=100\n",
    "        'subsample': hp.uniform('subsample', 0.60, 1.0), # default=1.0\n",
    "    #    'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2']), # default=None\n",
    "     #   'min_samples_split': hp.uniform('min_samples_split', 0.0, 1.0), # default=2 # NB! decided for using float instead of int\n",
    "     #  'min_samples_leaf': hp.uniform('min_samples_leaf', 0.0, 1.0), # default=1 # NB! decided for using float instead of int\n",
    "     #   'min_weight_fraction_leaf': hp.uniform('min_weight_fraction_leaf', 0.0, 0.5), # default=0.0\n",
    "        'max_depth': hp.choice('max_depth', range(1,20)), # default=3\n",
    "    #    'warm_start': hp.choice('warm_start', [True, False])\n",
    "        \n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# Dataframe for model names, classes, and search spaces\n",
    "df_models = pd.DataFrame({'name': names, 'classifier': classifiers, 'search_space' : search_spaces})\n",
    "df_models['n_hp'] = df_models['search_space'].map(len) # the number of hyperparameters in search space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db3199",
   "metadata": {},
   "source": [
    "Because some of the classifiers have very varying results depending on the random state, we run them trough 100 iterations to get their average accuracy score. We use accuracy score to compare the baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b485e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the number of iterations\n",
    "iterations = 100\n",
    "\n",
    "df_models2 = df_models.copy()\n",
    "bl_runtimes = {}\n",
    "bl_scores = {}\n",
    "# Iterate over all classifiers\n",
    "for i in range(len(df_models)):\n",
    "    classifier_name = df_models2['name'][i]\n",
    "    classifier = df_models2['classifier'][i]\n",
    "    \n",
    "    times = []\n",
    "    scores = []\n",
    "    # Compute the model accuracy and runtime for N iterations for each classifier\n",
    "    for j in range(iterations):\n",
    "        start_time = time.time() # track time\n",
    "        \n",
    "        # Train and test the model\n",
    "        classifier.fit(X_train, y_train)\n",
    "        score = classifier.score(X_test, y_test)\n",
    "        \n",
    "        end_time = time.time() # track time\n",
    "        totalTime = end_time - start_time\n",
    "        \n",
    "        scores.append(score)\n",
    "        times.append(totalTime)\n",
    "    \n",
    "    bl_runtimes[classifier_name] = times\n",
    "    bl_scores[classifier_name] = scores\n",
    "    \n",
    "# Transforming the results to long dataframes\n",
    "bl_runtimes = pd.DataFrame(bl_runtimes).transpose().reset_index()\n",
    "bl_runtimes = pd.melt(bl_runtimes, id_vars='index', var_name = 'iter')\n",
    "bl_runtimes.rename(columns = {'index':'name', 'value': 'runtime'}, inplace = True)\n",
    "\n",
    "bl_scores = pd.DataFrame(bl_scores).transpose().reset_index()\n",
    "bl_scores = pd.melt(bl_scores, id_vars='index', var_name = 'iter')\n",
    "bl_scores.rename(columns = {'index':'name', 'value': 'accuracy'}, inplace = True)\n",
    "# Merge into one table\n",
    "bl_results = bl_runtimes.merge(bl_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bdf0eb",
   "metadata": {},
   "source": [
    "### An overview of classifiers' baseline performance\n",
    "Below the performance of baseline models across 100 computations are displayed. Specifically, the figures show the distributions of (a) accuracy scores as well as (b) runtimes for each classifier across 100 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa2a7dc",
   "metadata": {},
   "source": [
    "#### Accuracy of baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data\n",
    "grouped_s = bl_scores.loc[:,['name', 'accuracy']] \\\n",
    "    .groupby(['name']) \\\n",
    "    .median() \\\n",
    "    .sort_values(by='accuracy', ascending = False)\n",
    "\n",
    "# Set the figure properties\n",
    "sns.set(rc={'figure.figsize':(6,4)})\n",
    "sns.set_style(\"whitegrid\") \n",
    "\n",
    "# Figure\n",
    "sns.boxplot(data= bl_scores, x=\"accuracy\", y=\"name\", \n",
    "            order = grouped_s.index,\n",
    "            flierprops={\"marker\": \"x\"},\n",
    "            boxprops={\"facecolor\": (.4, .6, .8, .5)},\n",
    "            medianprops={\"color\": \"coral\"}\n",
    "           ).set_title('Classifier accuracy: 100 iterations', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d823c3d6",
   "metadata": {},
   "source": [
    "#### Runtime of baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5096da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data\n",
    "grouped_rt = bl_runtimes.loc[:,['name', 'runtime']] \\\n",
    "    .groupby(['name']) \\\n",
    "    .median() \\\n",
    "    .sort_values(by='runtime', ascending = False)\n",
    "\n",
    "# Set the figure properties\n",
    "sns.set(rc={'figure.figsize':(6,4)})\n",
    "sns.set_style(\"whitegrid\") \n",
    "\n",
    "# Figure\n",
    "sns.boxplot(data= bl_runtimes, x=\"runtime\", y=\"name\", \n",
    "            order = grouped_rt.index,\n",
    "            flierprops={\"marker\": \"x\"},\n",
    "            boxprops={\"facecolor\": (.4, .6, .8, .5)},\n",
    "            medianprops={\"color\": \"coral\"}\n",
    "           ).set_title('Classifier Runtime: 100 iterations', fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef1cc13",
   "metadata": {},
   "source": [
    "Below we present the aggregated results (average and median) for each classifier across all iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491d44c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute the median and average accuracy and runtime; sort by decreasing accuracy and increasing runtime\n",
    "bl_agg = bl_results.groupby(['name']).agg({'accuracy': ['mean', 'median'], 'runtime': ['mean', 'median']})\n",
    "bl_agg.columns = ['_'.join(col) for col in bl_agg.columns]\n",
    "bl_agg = bl_agg[['accuracy_mean', 'runtime_mean']].sort_values(['accuracy_mean', 'runtime_mean'], ascending = [0,1]).reset_index()\n",
    "bl_agg = bl_agg.rename(columns = {'accuracy_mean': 'bl_acc', 'runtime_mean': 'bl_runtime'})\n",
    "bl_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a3900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2140b9db",
   "metadata": {},
   "source": [
    "<font color = 'red'> SIIA KIRJUTADA JÄRELDUSED: MIS OLID TOP ALGORITMID </font>\n",
    "\n",
    "Let us now zoom in on the five best classifiers and choose them for hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2f5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the best average accuracies and runtimes to the `df_models` dataframe defined earlier\n",
    "df_models = df_models2.merge(bl_agg).sort_values(['bl_acc', 'bl_runtime'], \n",
    "                                                ascending = [0,1]).reset_index(drop = True)\n",
    "\n",
    "\n",
    "# Choose the top 5 baseline classifiers\n",
    "top_k = 5 # can be changed if needed\n",
    "df_best_bl = df_models.iloc[:top_k] # choose only the top algos' aggregated results\n",
    "df_best_bl[['name', 'bl_acc', 'bl_runtime']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a4cddf",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "The following section is divided into two parts: (1) the `hyperopt` approach and the `hyperopt-sklearn` approach. The difference between these two is that while `hyperopt` generally requires user-defined, manually mapped search spaces for model hyperparameters, `hyperopt-sklearn` also provides the possibility for not manually defining the search spaces, and making the searches on its own automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798529a",
   "metadata": {},
   "source": [
    "### The `hyperopt` approach\n",
    "A great resource for hyperparameter optimization using the `hyperopt` library can be found [here](https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce). The `hyperopt` Python module implements Bayesion Hyperparameter Optimization (HPO), and within this module, one needs to define manually the search space for a given model/classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aafa7b",
   "metadata": {},
   "source": [
    "With the following codeblock, we are implementing the HPO by iterating over the selected classifiers and their manually-defined search spaces. The best classifiers as well as their potential search space is saved in the previous section into the `df_best_bl` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99caa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize a dataframe for results collection\n",
    "hpo_results = pd.DataFrame(columns = ['classifier_name', 'best_cv_score', 'runtime_hpo', 'best_params', 'test_acc'])\n",
    "\n",
    "# Define the best baseline score\n",
    "best_baseline_score = df_best_bl.loc[0, 'bl_acc']\n",
    "\n",
    "# Define how many iterations should be done\n",
    "n_trials = 10\n",
    "\n",
    "# All individual trials will be saved here\n",
    "trials_dict = {}\n",
    "\n",
    "# Start the timer to measure the runtime of the entire pipeline\n",
    "hpo_time_start = time.time()\n",
    "\n",
    "# Run the HPO and get the best params for each classifier\n",
    "for i in range(len(df_best_bl)):\n",
    "    \n",
    "    # To improve the raedability of code, creating the following objects:\n",
    "    classifier_name = df_best_bl.loc[i, 'name']\n",
    "    classifier_class = classifiers[names.index(classifier_name)] # fetch from the list not df for baseline\n",
    "    classifier_search_space = df_best_bl.loc[i, 'search_space']\n",
    "    \n",
    "  #  sp = search_spaces[names.index(df_best_bl.loc[i, 'name'])]\n",
    "    \n",
    "    # To improve the readability of output:\n",
    "    print()\n",
    "    print('----------------------------------------------------------------------')\n",
    "    print(f'Using *{classifier_name}* for estimation.')\n",
    "    print('----------------------------------------------------------------------')\n",
    "    \n",
    "    # A helper function for receiving the CV scores for each model\n",
    "    def hyperopt_cv_score(params):\n",
    "        cv = RepeatedKFold(n_splits=5, n_repeats=5) # can be also adjusted!\n",
    "        model = classifier_class.set_params(**params) # use the classifier from the list\n",
    "        return cross_val_score(model, \n",
    "                               X_train, y_train, \n",
    "                               cv = cv,  \n",
    "                               scoring = 'accuracy',\n",
    "                               error_score='raise').mean() \n",
    "\n",
    "    # A helper function for finding the best model\n",
    "    def f(params):\n",
    "        global best_cv_score\n",
    "        global best_params\n",
    "        global best_time\n",
    "        \n",
    "        cv_score = hyperopt_cv_score(params)\n",
    "    \n",
    "        if cv_score > best_cv_score:\n",
    "            # Are we bearing the best baseline score?\n",
    "            if cv_score > best_baseline_score: # we are beating the baseline accuracy\n",
    "                best_cv_score = cv_score # what is the best score?\n",
    "                best_params = params # what are the best params?\n",
    "                best_time = round(time.time() - start_time,4) # track how much time it took to find the best params\n",
    "                \n",
    "                print(f'*{classifier_name}* BEAT THE BASELINE of {best_baseline_score}!')\n",
    "                print(f'Better CV score: {best_cv_score}')\n",
    "                print(f'Parameter combination: {best_params}')\n",
    "                print(f'Time until beating the baseline: {best_time}s')\n",
    "            else: \n",
    "                best_cv_score = cv_score # what is the best score?\n",
    "                best_params = params # what are the best params?\n",
    "                best_time = round(time.time() - start_time,4) # track how much time it took to find the best params\n",
    "\n",
    "                # Print the results for reference\n",
    "                print(f'New best CV score: {best_cv_score}')\n",
    "                print(f'New best params: {best_params}')\n",
    "                print(f'Time taken until new best combination found: {round(best_time, 5)}s')\n",
    "\n",
    "        return {'loss': -cv_score, # see the comment below\n",
    "                'status': STATUS_OK}\n",
    "        # Comment regarding 'negative cv_score' (from the referenced source):\n",
    "        ## Since we are trying to maximize the CV score (cv_score in the code above), \n",
    "        ## we must negate this value for hyperopt, since hyperopt only knows how to minimize a function. \n",
    "        ## Minimizing a function f is the same as maximizing the negative of f.\n",
    "        ## About FMIN: https://github.com/hyperopt/hyperopt/wiki/FMin\n",
    "        \n",
    "    # Defining global variables to be updated\n",
    "    best_cv_score = 0 # best CV score\n",
    "    best_params = None # best hyperparameter combination\n",
    "    best_time = 0 # runtime until the best CV score is computed\n",
    "    trials = Trials() # store info at each step\n",
    "    \n",
    "    # Start running the algorithm and track time\n",
    "    start_time = time.time()\n",
    "    ## Objective function\n",
    "    best = fmin(f, \n",
    "                classifier_search_space, # use the search space associated with the classifier\n",
    "                algo = tpe.suggest, \n",
    "                max_evals = n_trials, # how many evaluations?\n",
    "                trials = trials)\n",
    "    # Save all trials\n",
    "    trials_dict[classifier_name] = trials # save into classifier-trials \n",
    "    \n",
    "    print()\n",
    "    print('######################################################################')\n",
    "    # Print the summary of the best results\n",
    "    print(f'Best CV score in {n_trials} iterations: {best_cv_score} ({best_time}s until found).')\n",
    "    \n",
    "    # Compute the accuracy score on the best model of the classifier\n",
    "    m = classifier_class.set_params(**best_params).fit(X_train, y_train)\n",
    "    score_test = m.score(X_test, y_test)\n",
    "    print(f'Accuracy score on test data: {score_test}.')\n",
    "    \n",
    "    \n",
    "    # Append the best results to the df\n",
    "    hpo_results = hpo_results.append({'classifier_name': classifier_name, \n",
    "                             'best_cv_score': best_cv_score, \n",
    "                             'runtime_hpo': best_time, \n",
    "                             'best_params': best_params,\n",
    "                             'test_acc': score_test}, ignore_index = True)\n",
    "\n",
    "# Mark the end of the entire pipeline\n",
    "hpo_time_end = time.time()\n",
    "print()\n",
    "print('######################################################################')\n",
    "print(f'The duration of the entire HPO pipeline for {len(df_best_bl)} classifiers across {n_trials} trials each: ')\n",
    "print(f'{round(hpo_time_end - hpo_time_start, 5)} seconds')\n",
    "\n",
    "# Sort the model results by test accuracy score and then Runtime\n",
    "hpo_results = hpo_results.sort_values(['test_acc', 'runtime_hpo'], ascending = [0,1]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5286ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a clumn which says if the classifier beat the baseline\n",
    "hpo_results['beats_bl'] = np.where(hpo_results.loc[:,'test_acc'] > best_baseline_score, 'yes', 'no')\n",
    "\n",
    "# See the HPO results\n",
    "hpo_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8295c",
   "metadata": {},
   "source": [
    "All of these models have generally fared off pretty well. Out of these solutions, which model reached their best score with the least amount of time and beat the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7817c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bl_beaters = hpo_results[(hpo_results['beats_bl'] == 'yes')]\n",
    "bl_beaters[bl_beaters['runtime_hpo'] == bl_beaters['runtime_hpo'].min()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910a2f4",
   "metadata": {},
   "source": [
    "Let's plot the model test set accuracy against runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5b7b8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = [5, 5])\n",
    "\n",
    "for i in range(len(hpo_results)):\n",
    "    # Add classifier values\n",
    "    plt.scatter(hpo_results['runtime_hpo'][i], \n",
    "            hpo_results['test_acc'][i], \n",
    "            label = hpo_results['classifier_name'][i],\n",
    "               marker = 'x',\n",
    "               s = 60)\n",
    "    # Add a dashed lined marking the best accuracy threshold for classifier\n",
    "    plt.hlines(y = hpo_results['test_acc'][i], \n",
    "               xmin=0, \n",
    "               xmax = np.max(hpo_results['runtime_hpo'])+1,\n",
    "              linestyles = 'dashed', alpha = 0.3, color = 'grey')\n",
    "    # Add classifier name\n",
    "    plt.annotate(hpo_results['classifier_name'][i], \n",
    "                 (hpo_results['runtime_hpo'][i]+1, \n",
    "                  hpo_results['test_acc'][i]),\n",
    "                )\n",
    "plt.ylim(bottom = 0.70, top = 1.01)\n",
    "plt.xlabel('Runtime (s)')\n",
    "plt.ylabel('Test accuracy')\n",
    "plt.title('Runtime until the best parameters were found')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6502ec",
   "metadata": {},
   "source": [
    "In addition to displaying the best accuracies and runtimes of each classifier, let's also look into how the accuracy scores were associated with different values of hyperparameters. For that, we first define a helper function that take classifier and its trials' related information as input and provides the parameter value-accuracy figure as output. We then iterate over all models to display the values of parameters in relation to accuracy scores accross all trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_plot(classifier_name, trials, parameters):\n",
    "    \"\"\"Parameters plot from trials\n",
    "    Args:\n",
    "        classifier_name (str): the name of the classifier\n",
    "        trials (trials obj): all trials from runs\n",
    "        parameters (list): list of parameters to be plotted\n",
    "    \n",
    "    Returns: \n",
    "        plot for parameter performance across trials\n",
    "    \"\"\"\n",
    "    # Define the figure (make a 2x3 grid universally)\n",
    "    f, axes = plt.subplots(nrows=2, ncols=3, figsize=(8,5))\n",
    "    f.suptitle(f'{classifier_name} HP performance across {len(trials)} trials') # set the plot title\n",
    "    f.tight_layout()\n",
    "\n",
    "    cmap = plt.cm.jet\n",
    "    for i, val in enumerate(parameters):\n",
    "        # Getting the hyperparameter and loss values with list comprehension\n",
    "        xs = np.array([t['misc']['vals'][val] for t in trials.trials]).ravel()\n",
    "        ys = [-t['result']['loss'] for t in trials.trials]\n",
    "        # Making the necessary sorting\n",
    "        xs, ys =  zip(*sorted(zip(xs, ys)))\n",
    "        ys = np.array(ys)\n",
    "\n",
    "        # Plot\n",
    "        axes[int(i/3),int(i%3)].scatter(xs, ys, \n",
    "                              s=10, \n",
    "                              linewidth=0.01, \n",
    "                             # alpha= 0.5,\n",
    "                              color=cmap(float(i)/len(parameters))\n",
    "                                       )\n",
    "        axes[int(i/3),int(i%3)].set_title(val)\n",
    "        axes[int(i/3),int(i%3)].set_ylim([0.2,1.0])\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf18bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(hpo_results)):\n",
    "    classifier_name = hpo_results.loc[i, 'classifier_name']\n",
    "    trials = trials_dict[classifier_name]\n",
    "    parameters = hpo_results.loc[i, 'best_params'].keys()\n",
    "    # figure\n",
    "    params_plot(classifier_name, trials, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75703c96",
   "metadata": {},
   "source": [
    "# Significance testing \n",
    "In the below section, we will test whether the differences between the models are statistically significant. For that, we will use pairwise McNemar test to compare each model with each other. P-values have the continuity correction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab85bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2c2324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sig_test_classifiers = [] # insantiate classifier name list\n",
    "cv_scores = {} # instantiate a dictionary for CV scores collection\n",
    "\n",
    "for i in range(len(hpo_results)):\n",
    "    # Remove spaces from classifier name\n",
    "    classifier_name = hpo_results.loc[i, 'classifier_name'].replace(' ', '')\n",
    "    \n",
    "    # Fit the model with the best params (are saved in the env), compute CV scores\n",
    "    model1 = classifiers[names.index(hpo_results.loc[i, 'classifier_name'])].set_params(**).fit(X_train, y_train)\n",
    "    folds5 = RepeatedKFold(n_splits = 5, n_repeats = 20, random_state = 2022) # 5 splits, 20 repeats = 100 samples per algo\n",
    "    scores = cross_val_score(model1, X_test, y_test, scoring = \"accuracy\", cv = folds5)\n",
    "    # Save cv scores and the classifier name\n",
    "    cv_scores[classifier_name] = scores\n",
    "    sig_test_classifiers.append(classifier_name)\n",
    "\n",
    "    \n",
    "# Repeat a similar process with the best baselines\n",
    "\n",
    "\n",
    "cv_scores = pd.DataFrame(cv_scores)\n",
    "cv_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb54d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friedman test\n",
    "stat, p = friedmanchisquare(cv_scores.iloc[:,0],\n",
    "                            cv_scores.iloc[:,1],\n",
    "                            cv_scores.iloc[:,2],\n",
    "                            cv_scores.iloc[:,3],\n",
    "                            cv_scores.iloc[:,4]\n",
    "                           )\n",
    "\n",
    "if p < 0.001:\n",
    "    print('Friedman chi-squared statistic = %.3f, p < 0.001.' % (stat))\n",
    "else:\n",
    "    print('Friedman chi-squared statistic = %.3f, p = %.3f' % (stat, p))\n",
    "    \n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Same distributions (fail to reject H0)')\n",
    "else:\n",
    "    print('Different distributions (reject H0).')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1891b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#perform Nemenyi post-hoc test\n",
    "nm_test = sp.posthoc_nemenyi_friedman(np.array(cv_scores))\n",
    "nm_test.columns = sig_test_classifiers\n",
    "nm_test.index = sig_test_classifiers\n",
    "\n",
    "print('Nemenyi post hoc test results (pairwise comparison p-values):')\n",
    "nm_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10947903",
   "metadata": {},
   "source": [
    "<font color = 'red'> SIIA MINGID COMMENTID </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d191f6",
   "metadata": {},
   "source": [
    "<font color = 'red'> JÄRGNEV HYPEROPT-SKLEARN OSA VÕIB OLLA TEG VÄGA OPTIONAL, VÕIME KA VÄLJA JÄTTA </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2dd412",
   "metadata": {},
   "source": [
    "## The `hyperopt-sklearn` approach\n",
    "In <font color = 'red'> SECTION </font>, we provided the definition for the search space of each selected classifier. However, `hyperopt` can also function without explicitly specifying the search space - enter `hyperopt-sklearn`, a presently developed library (see [here](https://github.com/hyperopt/hyperopt-sklearn)) that provides more automation for hyperparameter optimization. `hyperopt-sklearn` has support for a deecnt number of classifiers, it allows to automatically search for the best models from either within all possible model classes or from a user-defined set of models; it is possible to implement data preprocessing (a functionality that we decided not to use during the present project) as well as define the search space manually or let the computer decide it randomly based on all possible hyperparameters. \n",
    "\n",
    "Below we will run the classifiers used above without specified search space. Of note, this might mean that some models will not compute or converge, since some hyperparameter combinations do not work together. In those cases, we just skip the hyperparameter combination and proceed.\n",
    "\n",
    "The main advantage of `hyperopt-sklearn` is that we can reduce manual work in specifying hyperparameter space for each classifier. The main disadvantage is that some of the previously-used functions are not easily programmable here, i.e., the `hypropt-sklearn` is somewhat less flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58009fa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "--- to create an error for precventing auto-run when restarting and running the notebook :)\n",
    "    \n",
    "# 1. Initialize a dataframe\n",
    "model_results_auto = pd.DataFrame(columns = ['classifier', 'test_acc', 'best_params'])\n",
    "\n",
    "# 2. Classifier labels\n",
    "names_auto = [\n",
    "    \"Gradient Boosting Classifier\",\n",
    "    \"LDA\",\n",
    "    \"Random Forest\",\n",
    "    \"AdaBoost\",\n",
    "    \"Logistic Regression\"\n",
    "]\n",
    "\n",
    "# 3. Classifier classes\n",
    "classifiers_auto = [\n",
    "    gradient_boosting_classifier(\"gbc\"),\n",
    "    linear_discriminant_analysis('lda'),\n",
    "    random_forest_classifier('rf'), \n",
    "    ada_boost_classifier('ada'), \n",
    "    logistic_regression('logreg')\n",
    "]\n",
    "\n",
    "# 4. Iterate over all classifiers\n",
    "for i in range(len(classifiers_auto)):\n",
    "    print()\n",
    "    print(f'Using the *{names_auto[i]}* for estimation.')\n",
    "    \n",
    "    # Objective function\n",
    "    estim = HyperoptEstimator(classifier = classifiers_auto[i],\n",
    "                                    #  preprocessing=[],\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=100)\n",
    "    try:\n",
    "        # Fit the model\n",
    "        estim.fit(X_train, y_train)\n",
    "        \n",
    "        # Add the results to df\n",
    "        model_results_auto = model_results_auto.append(\n",
    "            {'classifier': names_auto[i], \n",
    "            # 'runtime_hpo': best_time,\n",
    "             'test_acc': estim.score(X_test, y_test),\n",
    "             'best_params': estim.best_model()\n",
    "             }, \n",
    "            ignore_index = True)\n",
    "    except:\n",
    "        print('Model with params not computed')\n",
    "        # Append the name but assign None-values to fields\n",
    "        model_results_auto = model_results_auto.append(\n",
    "            {'classifier': names_auto[i], \n",
    "            # 'runtime_hpo': best_time,\n",
    "             'test_acc': None,\n",
    "             'best_params': None\n",
    "             }, \n",
    "            ignore_index = True)\n",
    "\n",
    "model_results_auto = model_results_auto.sort_values(['test_acc'], ascending = False).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34a7ec",
   "metadata": {},
   "source": [
    "Look at the results table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765af402",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_results_auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be26ac9",
   "metadata": {},
   "source": [
    "## Bonus: Kaggle submission\n",
    "Below is a little piece of code for external model validation with a holdout test set on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7327f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "--- to create an error\n",
    "# Kaggle\n",
    "## Use the best model to predict the target\n",
    "bestm = GradientBoostingClassifier(**model_results['best_params'][0]).fit(X_train, y_train)\n",
    "pred_best = bestm.predict(X_test)\n",
    "\n",
    "# Prepare the submission df\n",
    "submission_best1 = pd.DataFrame(\n",
    "    {'station_id': X_test['station_id'], 'compliance_2021': pred_best},\n",
    "    columns = ['station_id', 'compliance_2021']\n",
    ").set_index('station_id')\n",
    "\n",
    "# Write to .csv (for manual data submission)\n",
    "submission_best1.to_csv('best_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
